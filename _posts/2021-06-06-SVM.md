---
toc: true
layout: post
description: The last blog you need to understand SVM
categories: [Machine Learning]
title: Understanding Support Vector Machine
---

In Machine Learning, out of the many available supervised classification algorithms, Support Vector Machine (SVM) is one of the easiest algorithm and yet sometimes it becomes difficult to grasp due to its little nuances. In this blog, I try to lay down my notes on unboxing the SVM blackbox and make it easy to understand in detail. This blog focusses on entirety of SVM, from its introduction to classification and kernel tricks.

The idea of SVM is simple: The algorithm creates a line or a hyperplane that separates data into different classes. Let's understand how SVM creates the hyperplane and what are its other jargon.

### What is SVM?
Support Vector Machine (SVM) is a supervised ML algorithm that can be used for classification and ***regression as well***. However, it is mostly used in classification problems. 

In SVM classificatio, each data point is plotted in an **n-dimensional** space (**n** = Nos. of features). and classified by finding the best hyperplane that differentiates the classes well.
![]({{ site.baseurl }}/images/svm_hyperplane.png "Fig: 1. SVM Example")

In the above image, data points are plotted in a 2-dimensional space and **solid black** line is the hyperplane that is able to divide the dataset in two classes viz. orange and green color. The hyperplane becomes the decision boundary where if datapoints are above the hyperplane( black line) they belong to <span style="color:orange">*orange*</span> class and if datapoints are below the hyperplane they belong to <span style="color:green">*green*</span> class.
{% include alert.html text="Classification is not done based on hyperplane decision boundary alone but based on hyperplane plus support vector decision region which is explained in next section."%}

{% include info.html text="Hyperplanes segregates the datasets into two classes only." %}Hyperplanes are **n-1 -dimensional** planes where **n** is total number of features of the dataset. From the above image, data is 2 dimensional and hyperplane divding the space is a 1 dimensional plane.

*Taking it further*: in the above example there is a hyperplane drew that differentiates the dataset in two classes very well. But on keenly thinking, there are other possible hyperplanes too which could be selected that divides the dataset. Something like in the image below:
![]({{ site.baseurl }}/images/svm_multi_hyperplane.png "Fig: 2. Multiple Hyperplanes")

Apart from the **black** coloured hyperplane line, there are other infinite hyperplanes available(to name a few: red, blue, yellow) which could differentiate the dataset too. But which is the best hyperplane among all and what is the criteria to call a hyperplane **best**? This question is answered with the help of **Support Vectors**.

#### What are Support Vectors?
Support Vectors are the vectors passing through the closest data points to the hyperplanes and are parallel to the hyperplane. The distance between the support vectors and hyperplane is called as **margin**. The thumb rule to identify the right hyperplane among infinitly available hyperplanes is: "***Select the hyper-plane which segregates the two classes better such that the margin is maximum between the support vector and hyperplane***".
![]({{ site.baseurl }}/images/svm_sv_margin.png "Fig: 3. Best Hyperplane with its Support Vector")

In above image, the middle solid black colored line is the hyperplane that divides the dataset into classes whereas two parallel dashed lines are its **support vectors** and they pass through the closest datapoints to the hyperplane.

The **margin** is maximum ever possible due to these chosen support vectors and hence the hyperplane derived from it is the best hyperplane to classify the datapoints into classes.

Few points on Support Vectors:

    1. The data/vector points closest to the hyperplane are known as **support vector** data points because only these two points are contributing to the result of SVM, other points are not.
    2. If a data point is not an support vector, removing it has no effect on the model.
    3. Deleting the support vector will change the position of the hyperplane. 

{% include info.html text="The larger the margin the better it is." %}

#### Intuition for Large Margins
In SVM, the classification is not done based on the hyperplane decision boundary but even support vectors are also considered in classification. The region between two support vectors which includes hyperplane becomes the decision region. In fig. 3, if datapoints are above the decision region towards right they will be classified into <span style="color:orange">*orange*</span> class and if datapoints are below the decision region towards left they will be classified into <span style="color:green">*green*</span> class.

Maximizing the margin seems good because data points near the decision region represent very uncertain classification decisions. For example, after training SVM we got a certain hyperplane with its support vectors such that the margin is very-very small. In this case SVM does not have confidence on its prediction of datapoints which are very close to the decsision region: there is almost a $50\%$ chance of the classifier deciding either way. Whereas a SVM classifier with a large margin makes no low certainty classification decisions. A large margin gives classifier safety margin where a slight error in measurement or a slight document variation will not cause a misclassification.

#### Mathematics for SVM
Lets go through the mathematics of SVM and understand SVM manages to select the hyperplane with maximum margin possible.

Generally, the equation of a line is given by: $y=ax+b$. We should define the hyperplane equation with this equation but however, we will define the equation of hyperplane in this form: **$W^TX = 0$**. Lets see how these two forms are related.

In hyperplane equation the names of the variables are in bold which means that that they are **vectors** and **$W^TX$** represent the inner product(i.e. dot product) of two vectors.

Note that **$$y=ax+b$$** is same as **$$y-ax-b=0$$**. We can write: **$W = \begin{bmatrix}-b\cr-a\cr1\end{bmatrix}$** and **$X = \begin{bmatrix}1 \cr x \cr y \end{bmatrix}$**. Then:

$$W^TX = -b \times (1) + (-a) \times x + 1 \times y \\ \\
    W^TX = y - ax -b
$$

The two equations are just different ways of expressing the same thing. It is interesting to note that $$w_o$$ is $$-b$$ which determines the intersection of the line with the vertical axis.

We use hyperplane equation **$W^TX$** instead as **$y=ax+b$** for two reasons:

1. it is easier to work in more than two dimensions with this notation,
2. the vector **$W$** will always be normal to hyperplane (**$W$** will always be normal because it is used to define the hyerplane, so by definition it will be normal. [More Info Here](http://tutorial.math.lamar.edu/Classes/CalcII/EqnsOfPlanes.aspx))

##### Compute distance from a point to the hyperplane
In Fig. 4, there is a hyperplane that divides two sets of data. Here, there is no Y-intercept i.e. **$w_0 = 0$**.

![]({{ site.baseurl }}/images/svm_maths_1.png "Fig: 4.")

As can be seen from Fig. 4, the equation of hyperplane is: $$x_2 = -2x_1$$ which is equivalent to **$$W^TX = 0$$** with **$W = \begin{bmatrix}2\cr1\end{bmatrix}$** and **$X = \begin{bmatrix}x_1\cr x_2\end{bmatrix}$**. Note that vector **$W$** is shown in the figure. 

We would like to compute the distance between point **$A(3,4)$** and the hyperplane. This is the distance between **$A$** and its projection onto the hyperplane.

![]({{ site.baseurl }}/images/svm_maths_2.png "Fig: 45.")

We can view point **$A$** as a vector from the **origin** to **$A$**. If we project this vector **$A$** onto **$W$** we get vector **$P$**. The distance between point **$A(3,4)$** and hyperplane is same as $\|\|p\|\|$. **We will be finding $\|\|p\|\|$.**

Let **$u$** denote the direction of vector **$W$** which is given by $u = \frac{W}{\|\|w\|\|}$.

$$\|w\| = \sqrt{2^2 + 1^2} = \sqrt{5}$$. So,  **$u$** becomes $u = \begin{bmatrix}\frac{2}{\sqrt{5}}, \frac{1}{\sqrt{5}}\end{bmatrix}$.

To compute $\|\|p\|\|$, we need to find value of vector **$P$**. From, the figure we can conclude that **$P$** is the orthogonal projection of **$A$** onto **$W$**. 

The dot product between **$A$** and **$W$** is: $A.W = \|\|a\|\| \times \|\|w\|\| \cos\theta \Longrightarrow \cos\theta = \frac{A.W}{\|\|a\|\| \times \|\|w\|\|}$

If we apply, Pythagoras theorem in the above figure we will also get: $\cos\theta = \frac{\|\|p\|\|}{\|\|a\|\|}$. 

Equating above two equations:

$$
\begin{align}
    \frac{A.W}{\|a\| \times \|w\|} = \frac{\|p\|}{\|a\|} \\

    \frac{A.W}{\|w\|} = \|p\| \\
\end{align}
$$

We already know that:

$$
    u = \frac{W}{\|w\|}. & So, \\ \\

    \|p\| = u . A
$$

Vector **$P$** is in same direction as vector **$W$**. Hence, we can define $u$ in terms of vector **$p$** as well:

$$
u = \frac{P}{\|p\|} \Longrightarrow P = \|p\| \times u \\
P = (u.A) \times u
$$
