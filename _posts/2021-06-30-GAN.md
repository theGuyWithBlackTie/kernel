---
toc: true
layout: post
description: The introduction blog to start in GAN world
title: Generative Adversarial Networks
hide: true
---
We have come across various articles and posts about AI being capable of producing human like speech or generating images of non-existing people that are difficult to distinguish from real-life existence. These AI systems are built upon generative adversarial networks (GANs) - which Facebook AI research director Yann LeCun called "*most interesting idea in last 10 years in ML*" GANs were introduced in a [paper](!https://arxiv.org/abs/1406.2661) by Ian Goodfellow and other researchers at the University of Montreal, including Yoshua Bengio, in 2014.

GANs' potential for both good and evil is huge, because they can learn to mimic any distribution of data. That is, GANs can be taught to create worlds eerily similar to our own in any domain: images, music, speech, prose. They are robot artists in a sense, and their output is impressive - poignant even. But they can also be used to generate fake media content and are the technology underpinning Deepfakes.

To understand GANs, we need to understand generative algorithms and its counterpart discriminative algorithms.

## Background: Generative Models v/s Discriminative Models

### Discriminative Models
Discriminative models classifies the input data; i.e. given the features of an instance of data, they predict the label or cateogry to which the data belongs. For example, given , all the words in an email (this is data instance), a discriminative model would predict whether the email is `spam` or `not_spam`. `spam` is one of the labels, and the bag of words gathered from the email are the features that constitute the input data. When this problem is expressed mathematically, the label is called as $Y$ and the features are called as $X$. The formualtion $P(Y|X)$ is used to mean "*the probability of y given x*" which in this case would translate to "the probability that an email is spam given the words it contains".

So discriminative models maps features to labels. They are concerned solely with that correlation. Discriminative models directly learns the conditional distribution $P(Y\|X)$.

### Generative Models
On the other hand, generative models learns the joint probability distribution $P(X,Y)$ by explicitly learning $P(X|Y)$ and $P(Y)$, and then uses it to compute $P(Y|X)$ through 
Bayes rule.

$$
Given \space a \space model \space of \space the \space joint \space distribution, \space P(X,Y), \space marginal \space distribution \space of \space X \space \& \space Y \space can \space be \space calculated \space as: \\
P(X) = \sum_{y}P(X, Y = y) \space \space and \\
P(Y) = \int_{x}P(Y, X = x)
$$

considering $X$ as continuous, hence integrating over it and $Y$ as discrete hence summing over it.

Either conditional distribution can be computed as:

$$
P(Y|X) = \frac{P(X,Y)}{P(X)}
$$

Thats how, we can compute $P(Y\|X)$ in generative models.


<span class = "highlight">*One way think about generative models from GANs perspective is that they do the opposite of discriminative models.*</span> Instead of predicting labels given the features, generative models attempt to predict features given a certain label i.e. $P(X\|Y)$. Understanding it from the email example, generative model tries to answer this: Assuming the given eamil is `spam`, how likely are these features? While discriminative models care about relation between $Y$ and $X$, generative models care about <span class = "highlight">"how you get $X$?"</span>

Formal Definitions:

* A **discriminative** model learns a function that maps the input data `X` to some desired output class label `Y`. In probabilistic terms, they directly learn the conditional distribution `P(Y|X)`.
* A **generative** model tries to learn joint probability of the input data and labels simultaneously, i.e. `P(X,Y)`. This is converted to `P(X|Y)` for classification via Bayes rule, but the generative ability could be used for something else as well, such as creating new `(X,Y)` sample.

    ```The word "generative" in generative models does not mean that the model generates actual new data additionally to the dataset. It refers to the nature of the theoretical 
    model, in the sense that the generative approach assumes that any sample of data in generated from some distribution, and it tries to estimate this distribution. Once 
    the distribution is estimated the model could be used to actually generate instances following this distribution.```

Both these models are used in supervised learning where one wants to learn a rule that maps input $X$ to output $Y$. Some argues that the discriminative models are better as they directly models the quantity we care about i.e. $Y$ and hence no efforts are spent on modelling the input $X$. However, generative models has its own advantages such as capability of dealing with missing data. Since, generative models concerns about $P(X,Y)$ and $P(X)$ at the same time in order to predict $P(Y\|X)$, they have less **degree of freedom** as compared to discriminative models. So generative models are more robust, less prone to overfitting than discriminative models.

### Generative Models are hard
As explained earlier, generative models are better than discriminative models in certain aspects but they are even difficult to train. Generative models tackle a more difficult task than analogous discriminative models. Generative models have to model more.

A generative model for images might capture correlations like "things that look like boats are probably going to appear near things that look like water" and "eyes are unlikely to appear on foreheads". These are very complicated distributions.

In contrast, a discriminative model might learn the difference between "sailboat" or "not sailboat" by just looking for a few tell-tale patterns. It could ignore many of the correlations that the generative models ***must*** get right.

Discriminative models try to draw boundaries in the data space, while generative models try to model how data is placed throughut the space. For example, the following diagrams shows discriminative and generative models of handwritten digits:

![]({{ site.baseurl }}/images/GAN_1.png "Fig: 1. Discriminative and Generative Model of handwritten digits")

The discriminative model tries to tell the difference between handwritten 0's and 1's by drawing a line in the data space. If it gets the line right, it can distinguish 0's from 1's without ever having to model exactly where the instances are placed in the data space on either side of the line.

In contrast, the generative model tries to produce convincing 1's and 0's by generating digits that fall close to their real counterparts in the data space. It has to model the distribution throughout the data space. GANs offer an effective way to train such rich models to resemble a real distribution.


**Refereces**
1. [A Beginner's Guide to Generatvive Adversarial Networks (GANs)](!https://wiki.pathmind.com/generative-adversarial-network-gan)
2. [Stackoverflow](!https://stats.stackexchange.com/questions/4689/generative-vs-discriminative-models-in-bayesian-context)
3. [Generative Adversarial Networks - Google Developers](!https://developers.google.com/machine-learning/gan)



