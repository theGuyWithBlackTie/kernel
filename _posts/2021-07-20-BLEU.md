---
toc: true
layout: post
description: A blog explaining everything about BLEU score in Neural Machine Translation
title: What is BLEU score?
categories: [Deep Learning]
hide: true
---

BLEU, or Bilingual Evaluation Understudy, is a performance metric score for comparing the machine translations and human created reference translations for the same source sentence. This metric was introduced in Kishore Papineni, et al. in their 2002 paper "[BLEU: a Method for Automatic Evaluation of Machine Translation](!https://aclanthology.org/P02-1040.pdf)"

The goal of any machine translation system is to produce translation results equal to those of a professional human translator. BLEU algorithm is designed to measure eactly that.

A very easy metric to score machine translation result is to compute its *precision*. To compute precision, one simply counts the number of candidate translation words (i.e. unigrams) which occur in any reference translation and then divides by the total number of words in the candidate translation.

Let's take an example:

**Candidate Sentence:** there is cat on mat <br>
**Reference Sentence:** the cat is on the mat

Total nos. of unigrams in candidate sentence is $5$. Whereas, only 3 unigrams matches with the unigrams in the reference sentence. Hence precision would be: $3/5 = 0.6$

On a closer look, we can see precision is not a very good metric to evaluate the machine translations. Machines (or Neural Networks) can over predict "reasonable" words, resulting in improbable, but high-precision translations just like below example:

**Candidate Sentence:** the the the the the the<br>
**Reference Sentence:** the cat is on the mat

Similar to previous example, we can calculate the precision for candidate sentence which would be $5/5$ but the translation is terrible.

## Modified n-gram precision
The core of the BLEU metric score is precision. But we saw a regular precision metric is not a right metric to evaluate the translation quality and could give better scores to a very horrendous translation. Due to this, *modified n-gram* precision is introduced in BLEU and it makes the core of the BLEU metric.

Let's understand *modified **unigram** precision* first to get better idea. To compute this, one firsts counts the maximum number of times a word occurs in any single reference transation. Next, one clips the total count of each candidate word by its maximum reference count, add these clipped counts up, and divides by the total (unclipped) number of candidate words.

**Candidate Sentence:** the the the the the the the<br>
**Reference Sentence 1:** the cat is on the mat<br>
**Reference Sentence 2:** There is a cat on the mat<br>

With respect to reference sentence 1 the frequency of words are:
*the:* 2<br>
*cat:* 1<br>
*is:* 1<br>
*on:* 1<br>
*mat:* 1<br>
