---
toc: true
layout: post
description: A blog explaining everything about BLEU score in Neural Machine Translation
title: What is BLEU score?
categories: [Deep Learning]
hide: true
---

BLEU, or Bilingual Evaluation Understudy, is a performance metric score for comparing the machine translations and human created reference translations for the same source sentence. This metric was introduced in Kishore Papineni, et al. in their 2002 paper "[BLEU: a Method for Automatic Evaluation of Machine Translation](!https://aclanthology.org/P02-1040.pdf)"

The goal of any machine translation system is to produce translation results equal to those of a professional human translator. BLEU algorithm is designed to measure eactly that.

A very easy metric to score machine translation result is to compute its *precision*. To compute precision, one simply counts the number of candidate translation words (i.e. unigrams) which occur in any reference translation and then divides by the total number of words in the candidate translation.

Let's take an example:

`**Candidate Sentence:** there is cat on mat 
**Reference Sentence:** the cat is on the mat
`
Total nos. of unigrams in candidate sentence is $5$. Whereas, only 3 unigrams matches with the unigrams in the reference sentence. Hence precision would be: $3/5 = 0.6$
