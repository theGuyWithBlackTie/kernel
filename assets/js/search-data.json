{
  
    
        "post0": {
            "title": "Understanding Cross-Entropy Loss and Focal Loss",
            "content": "In this blogpost we will understand cross-entropy loss and its various different names. Later in the post, we will learn about Focal Loss, a successor of Cross-Entropy(CE) loss that performs better than CE in highly imbalanced dataset setting. We will also implement Focal Loss in PyTorch. . Cross-Entropy loss has its different names due to its different variations used in different settings but its core concept (or understanding) remains same across all the different settings. Cross-Entropy Loss is used in a supervised setting and before diving deep into CE, first let’s revise widely known and important concepts: . Classifications . Multi-Class Classification One-of-many classification. Each data point can belong to ONE of CCC classes. The target (ground truth) vector ttt will be a one-hot vector with a positive class and C−1C-1C−1 negative classes. All the CCC classes are mutually exclusives and no two classes can be positive class. The deep learning model will have CCC output neurons depicting probability of each of the CCC class to be positive class and it is gathered in a vector sss (scores). This task is treated as a single classification problem of samples in one of CCC classes. . Multi-Label Classification Each data point can belong to more than one class from CCC classes. The deep learning model will have CCC output neurons. Unlike in multi-class classification, here classes are not mutually exclusive. The target vector ttt can have more than a positive class, so it will be a vector of 000s and 111s with CCC dimensionality where 000 is negative and 111 is positive class. One intutive way to understand multi-label classification is to treat multi-label classification as CCC different binary and independent classification problem where each output neuron decides if a sample belongs to a class or not. . Output Activation Functions . These functions are transformations applied to vectors coming out from the deep learning models before the loss computation. The outputs after transformations represents probabilities of belonging to either one or more classes based on multi-class or multi-label setting. . Sigmoid It squashes a vector in the range (0,1)(0,1)(0,1). It is applied independently to each element of vector sss. . f(si)=11+e−sif(s_i) = frac{1}{1 + e^{-s_{i}}}f(si​)=1+e−si​1​ . Softmax It squashes a vector in the range (0,1)(0, 1)(0,1) and all the resulting elements add up to 111. It is applied to the output vector sss. The Softmax activation cannot be applied independently to each element of vector s, since it depends on all elements of sss. For a given class sis_isi​, the Softmax function can be computed as: . f(s)i=e(si)∑jCesjf(s)_i = frac{e^{(s_i)}}{ sum_{j}^C e^{s_j}}f(s)i​=∑jC​esj​e(si​)​ . Losses . Cross Entropy Loss . The cross-entropy loss is defined as: . CE=−∑iCtilog(si)CE = - sum_i^C t_i log(s_i )CE=−i∑C​ti​log(si​) . where tit_iti​ and sis_isi​ are the goundtruth and output score for each class i in C. . Taking a very rudimentary example, consider the target(groundtruth) vector t and output score vector s as below: . Target Vector: [0.6 0.3 0.1] Score Vector: [0.2 0.3 0.5] . Then CE will be computed as follows: . CE = -(0.6)log(0.2) - 0.3log(0.3) - 0.1log(0.5) = 0.606 . . In supervised machine learning settings, elements of target vectors are either 1 or 0. The above example shows how CE is computed and how it is also applicable to find loss between the distributions. Categorical Cross-Entropy Loss . In multi-class setting, target vector t is one-hot encoded vector with only one positive class (i.e.ti=1t_i = 1ti​=1) and rest are negative class (i.e. ti=0t_i = 0ti​=0). Due to this, we can notice that losses for negative classes are always zero. Hence, it does not make much sense to calculate loss for every class. Whenever our target (ground truth) vector is one-hot vector, we can ignore other labels and utilize only on the hot class for computing cross-entropy loss. So, Cross-Entropy loss becomes: . CE=−log(si)CE = -log(s_i)CE=−log(si​) . . The above form of cross-entropy is called as Categorical Cross-Entropy loss. In multi-class classification, this form is often used for simplicity. The Categorical Cross-Entropy loss is computed as follows: . f(s)i=esi∑jCesj⇒CE=−∑iCtilog(f(s)i)⇒CE=−log(f(s)i)f(s)_i = frac{e^{s_i}}{ sum_{j}^{C}e^{s_j}} Rightarrow CE = - sum_i^C t_i log(f(s)_i) Rightarrow CE = -log(f(s)_i)f(s)i​=∑jC​esj​esi​​⇒CE=−i∑C​ti​log(f(s)i​)⇒CE=−log(f(s)i​) . As, SoftMax activation function is used, many deep learning frameworks and papers often called it as SoftMax Loss as well. . Binary Cross-Entropy Loss . Based on another classification setting, another variant of Cross-Entropy loss exists called as Binary Cross-Entropy Loss(BCE) that is employed during binary classification (C = 2). Binary classification is multi-class classification with only 2 classes. To dumb it down further, if one class is a negative class automatically the other class becomes positive class. In this classification, the output is not a vector s but just a single value. Let’s understand it further. . The target(ground truth) vector for a random sample contains only one element with value of either 1 or 0. Here, 1 and 0 represents two different classes (C = 2). The output score value ranges from 0 to 1. If this value is closer to 1 then class 1 is being predicted and if it is closer to 0, class 0 is being predicted. . $$BCE = - sum_{i=1}^{C=2}t_ilog(f(s)_i) = -t_1log(f(s_1)) - (1-t_1)log(1-f(s_1))$$ . s1s_1s1​ and t1t_1t1​ are the score and groundtruth label for the class CiC_iCi​ in CCC. s2=1−s1s_2 = 1 -s_1s2​=1−s1​ and t2=1−t1t_2 = 1 - t_1t2​=1−t1​ are the score and groundtruth label for the class C2C_2C2​. If t1=0t_1 = 0t1​=0 then −t1log(f(s1))-t_1log(f(s_1))−t1​log(f(s1​)) would become 000 and (1−t1)log(1−f(s1))(1-t_1)log(1-f(s_1))(1−t1​)log(1−f(s1​)) would become active. Similarly, if t1=1t_1 = 1t1​=1 then −t1log(f(s1))-t_1log(f(s_1))−t1​log(f(s1​)) would become active and (1−t1)log(1−f(s1))(1-t_1)log(1-f(s_1))(1−t1​)log(1−f(s1​)) would become 000. The loss can be expressed as: . CE={−log(f(s1))ift1=1−log(1−f(s1))ift1=0CE = begin{cases} -log(f(s_1)) &amp; if &amp; t_1 = 1 -log(1-f(s_1)) &amp; if &amp; t_1 = 0 end{cases}CE={−log(f(s1​))−log(1−f(s1​))​ifif​t1​=1t1​=0​ . To get the output score value between [0,1], sigmoid activation function is used. . . Due to the using of Sigmoid Activation function it is also called as Sigmoid-Cross Entropy Loss. . f(s1)=11+e−s1⇒CE=−t1log(f(s1))−(1−t1)log(1−f(s1))f(s_1) = frac{1}{1+e^{-s_1}} Rightarrow CE = -t_1log(f(s_1)) - (1-t_1)log(1-f(s_1))f(s1​)=1+e−s1​1​⇒CE=−t1​log(f(s1​))−(1−t1​)log(1−f(s1​)) . Cross-Entropy in Multi-Label Classification . As described earlier, in multi-label classification each sample can belong to more than one class. With CCC different classes, multi-label classification is treated as CCC different independent binary classification. Multi-label classification is a binary classification problem w.r.t. every class. The output is vector sss consisting of CCC number of elements. Binary Cross-Entropy Loss is employed in Multi-Label classification and it is computed for each class in each sample. . Loss &amp; per &amp; sample = sum_{i=1}^{i=C}BCE(t_i, f(s)i) = sum{i=1}^{i=C}t_ilog(f(s)_i) . Focal Loss . Focal Loss was introduced in Focal Loss for Dense Object Detection paper by He et al (at FAIR). Object detection is one of the most widely studied topics in Computer Vision with a major challenge of detecting small size objects inside images. Object detection algorithms evaluate about 10410^4104 to 10510^5105 candidate locations per image but only a few locations contains objects and rest are just background objects. This leads to class imbalance problem. . Using Binary Cross-Entropy Loss for training with highly class imbalance setting doesn’t perform well. BCE needs the model to be confident about what it is predicting that makes the model learn negative class more easily they are heavily available in the dataset. In short, model learns nothing useful. This can be fixed by Focal Loss, as it makes easier for the model to predict things without being 80−10080-100%80−100 sure that this object is something. Focal Loss allows the model to take risk while making predictions which is highly important when dealing with highly imbalanced datasets. . . Though Focal Loss was introduced with object detection example in paper, Focal Loss is meant to be used when dealing with highly imbalanced datasets. How Focal Loss Works? . Focal Loss is am improved version of Cross-Entropy Loss that tries to handle the class imbalance problem by down-weighting easy negative class and focussing training on hard positive classes. In paper, Focal Loss is mathematically defined as: . FocalLoss=−αt(1−pt)γlog(pt)Focal Loss = - alpha_t(1 - p_t)^{ gamma}log(p_t)FocalLoss=−αt​(1−pt​)γlog(pt​) . . The above definition is Focal Loss for only one class. It has omitted the $$ sum$$ that would sum over all the classes $$C$$. To calculate total Focal Loss per sample, sum over all the classes. What is Alpha and Gamma ? . The only difference between original Cross-Entropy Loss and Focal Loss are these hyperparameters: alpha(α alphaα) and gamma(γ gammaγ). Important point to note is when gamme=0 gamme = 0 gamme=0, Focal Loss becomes Cross-Entropy Loss. . Let’s understand the graph below which shows what influences hyperparameters α alphaα and γ gammaγ has on Focal Loss and in turn understand them. In the graph, “blue” line represents Cross-Entropy Loss. The X-axis or “probability of ground truth class” (let’s call it pt) is the probability that the model predicts for the ground truth object. As an example, let’s say the model predicts that something is a bike with probability 0.60.60.6 and it actually is a bike. In this case, pt is 0.60.60.6. In the case when object is not a bike, the pt is 0.4(1−0.6)0.4 (1-0.6)0.4(1−0.6). The Y-axis denotes the loss values at a given ‘pt‘`p_t`‘pt​‘. . As can be seen from the image, when the model predicts the ground truth with a probability of 0.60.60.6, the Cross-Entropy Loss is somewhere around 0.50.50.5. Therefore, to reduce the loss, the model would have to predict the ground truth class with a much higher probability. In other words, Cross-Entropy Loss asks the model to be very confident about the ground truth prediction. . This in turn can actually impact the performance negatively: . The deep learning model can actually become overconfident and therefore, the model wouldn’t generalize well. . Focal Loss helps here. As can be seen from the graph, Focal Loss with γ&gt;1 gamma &gt; 1γ&gt;1 reduces the loss for “well-classified examples” or examples when the model predicts the right thing with probability &gt;0.5&gt; 0.5&gt;0.5 whereas, it increases loss for “hard-to-classify examples” when the model predicts with probability &lt;0.5&lt;0.5&lt;0.5. Therefore, it turns the model’s attention towards the rare class in case of class imbalance. . γ gammaγ controls the shape of curve. The higher the value of γ gammaγ, the lower the loss for well-classified examples, so we could turn the attention of the model towards ‘hard-to-classify’ examples. Having higher γ gammaγ extends the range in which an example receives low loss. . Another way, apart from Focal Loss, to deal with class imbalance is to introduce weights. Give high weights to the rare class and small weights to the common classes. These weights are referred as α alphaα. . Why Focal Loss worked?? . Let’s try to understand why Focal Loss worked and BCE did not. The most important part of Focal Loss to understand is below graph: .",
            "url": "https://theguywithblacktie.github.io/kernel/machine%20learning/2021/05/20/cross-entropy-loss.html",
            "relUrl": "/machine%20learning/2021/05/20/cross-entropy-loss.html",
            "date": " • May 20, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Adding Variable Number of Layers in Neural Network",
            "content": "Consider following code block that defines a fixed 2-layer neural network. Imagine a scenario, where the network has huge number of layers, and typing out each layer manually is just not feasible. An even more notable scenario is when the number of layers of network are not fixed and it depends on some other conigurations. This article deals with these scenarios and lays out solution. . class Net(nn.Module): def __init__(self, input_dim, output_dim): super(Net, self).__init__() linear_layer_one = nn.Linear(input_dim, output_dim) linear_layer_two = nn.Linear(input_dim, output_dim) def forward(self, input): output = linear_layer_two( linear_layer_one(input) ) return output . Recently, I was implementing a library related to Graph Networks in PyTorch framework. There I encountered the second scenario where the number of layers in the neural network were not fixed and it network required number of layers to be added as an input. The user of the library would specify how many layers is required as the input and that many layers would be added in the neural network. . As a daily user of Python, my first solution was to use a list data structure with a for loop to add n-number of layers, like below code block. . class Net(nn.Module): def __init__(self, input_dim, output_dim, nos_linear_layer): super(Net, self).__init__() self.nn_layers = [] for i in range(0,nos_linear_layer): linear_layer = nn.Linear(input_dim, output_dim) self.nn_layers.append(linear_layer) def forward(self, input): outputs = None for i,layer in enumerate(self.nn_layers): outputs = layer(input) outputs = torch.nn.functional.Softmax(outputs, 1) return outputs . Above code would look correct and would be expected to run without any issue. But, the main issue is that the linear layers stored in Python list would not be trained. On calling model.parameters(), PyTorch would simply ignore the parameters of linear layers stored in the Python list. . The correct way is to use PyTorch’s list nn.ModuleList. . class Net(nn.Module): def __init__(self, input_dim, output_dim, nos_linear_layer): super(Net, self).__init__() self.nn_layers = nn.ModuleList() for i in range(0,nos_linear_layer): linear_layer = nn.Linear(input_dim, output_dim) self.nn_layers.append(linear_layer) def forward(self, input): outputs = None for i,layer in enumerate(self.nn_layers): outputs = layer(input) outputs = torch.nn.functional.Softmax(outputs, 1) return outputs .",
            "url": "https://theguywithblacktie.github.io/kernel/pytorch/2021/05/14/vary-layers-pytorch.html",
            "relUrl": "/pytorch/2021/05/14/vary-layers-pytorch.html",
            "date": " • May 14, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://theguywithblacktie.github.io/kernel/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://theguywithblacktie.github.io/kernel/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is Ashish! While working in software engineering for 3 years, I pondered upon Machine Learning course by Andrew Ng and since then my daily learnings have never been the same. Machine Learning or AI (as many calls it for sophistication) is a vast field that is growing exponentially even today. Hence, I am documenting my learning notes in this blog to help out people and me in future looking for answers. Reach out to me on my social handles or via email incase you have any questions or find errors. Thanks :) . If you like any of my blog or found it helpful in your ML journey, do let me know by messaging me on social media. Otherwise you could also share your appreciation by sharing the work on social media and taging me along with it. These would really encourage me to write even more quality blogs. .",
          "url": "https://theguywithblacktie.github.io/kernel/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://theguywithblacktie.github.io/kernel/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}