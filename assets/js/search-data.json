{
  
    
        "post0": {
            "title": "Understanding Support Vector Machine",
            "content": "In Machine Learning, out of the many available supervised classification algorithms, Support Vector Machine (SVM) is one of the easiest algorithm and yet sometimes it becomes difficult to grasp due to its little nuances. In this blog, I try to lay down my notes on unboxing the SVM blackbox and make it easy to understand in detail. This blog focusses on entirety of SVM, from its introduction to classification and kernel tricks. . The idea of SVM is simple: The algorithm creates a line or a hyperplane that separates data into different classes. Let’s understand how SVM creates the hyperplane and what are its other jargon. . What is SVM? . Support Vector Machine (SVM) is a supervised ML algorithm that can be used for classification and regression as well. However, it is mostly used in classification problems. . In SVM classificatio, each data point is plotted in an n-dimensional space (n = Nos. of features). and classified by finding the best hyperplane that differentiates the classes well. . In the above image, data points are plotted in a 2-dimensional space and solid black line is the hyperplane that is able to divide the dataset in two classes viz. orange and green color. The hyperplane becomes the decision boundary where if datapoints are above the hyperplane( black line) they belong to orange class and if datapoints are below the hyperplane they belong to green class. . . Classification is not done based on hyperplane decision boundary but based on hyperplane plus support vector decision region which is explained in next section. . Hyperplanes segregates the datasets into two classes only. Hyperplanes are n-1 -dimensional planes where n is total number of features of the dataset. From the above image, data is 2 dimensional and hyperplane divding the space is a 1 dimensional plane. . Taking it further: in the above example there is a hyperplane drew that differentiates the dataset in two classes very well. But on keenly thinking, there are other possible hyperplanes too which could be selected that divides the dataset. Something like in the image below: . Apart from the black coloured hyperplane line, there are other infinite hyperplanes available(to name a few: red, blue, yellow) which could differentiate the dataset too. But which is the best hyperplane among all and what is the criteria to call a hyperplane best? This question is answered with the help of Support Vectors. . What are Support Vectors? . Support Vectors are the vectors passing through the closest data points to the hyperplanes and are parallel to the hyperplane. The distance between the support vectors and hyperplane is called as margin. The thumb rule to identify the right hyperplane among infinitly available hyperplanes is: “Select the hyper-plane which segregates the two classes better such that the margin is maximum between the support vector and hyperplane”. . In above image, the middle solid black colored line is the hyperplane that divides the dataset into classes whereas two parallel dashed lines are its support vectors and they pass through the closest datapoints to the hyperplane. . The margin is maximum ever possible due to these chosen support vectors and hence the hyperplane derived from it is the best hyperplane to classify the datapoints into classes. . . The larger the margin the better it is. Intuition for Large Margins . Due to the support vectors, the classification is not done based on the hyperplane decision boundary but now even support vectors are also considered in classification. The region between two support vectors which includes hyperplane becomes the decision region. In fig. 3, if datapoints are above the decision region towards right they will be classified into orange class and if datapoints are below the decision region towards left they will be classified into green class. . Maximizing the margin seems good because data points near the decision region represent very uncertain classification decisions. For example, after training SVM we got a certain hyperplane with its support vectors such that the margin is very-very small. In this case SVM does not have confidence on its prediction of datapoints which are very close to the decsision region: there is almost a 50% chance of the classifier deciding either way. Whereas a SVM classifier with a large margin makes no low certainty classification decisions. A large margin gives classifier safety margin where a slight error in measurement or a slight document variation will not cause a misclassification. .",
            "url": "https://theguywithblacktie.github.io/kernel/machine%20learning/2021/06/06/SVM.html",
            "relUrl": "/machine%20learning/2021/06/06/SVM.html",
            "date": " • Jun 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Understanding Cross-Entropy Loss and Focal Loss",
            "content": "In this blogpost we will understand cross-entropy loss and its various different names. Later in the post, we will learn about Focal Loss, a successor of Cross-Entropy(CE) loss that performs better than CE in highly imbalanced dataset setting. We will also implement Focal Loss in PyTorch. . Cross-Entropy loss has its different names due to its different variations used in different settings but its core concept (or understanding) remains same across all the different settings. Cross-Entropy Loss is used in a supervised setting and before diving deep into CE, first let’s revise widely known and important concepts: . Classifications . Multi-Class Classification One-of-many classification. Each data point can belong to ONE of CCC classes. The target (ground truth) vector ttt will be a one-hot vector with a positive class and C−1C-1C−1 negative classes. All the CCC classes are mutually exclusives and no two classes can be positive class. The deep learning model will have CCC output neurons depicting probability of each of the CCC class to be positive class and it is gathered in a vector sss (scores). This task is treated as a single classification problem of samples in one of CCC classes. . Multi-Label Classification Each data point can belong to more than one class from CCC classes. The deep learning model will have CCC output neurons. Unlike in multi-class classification, here classes are not mutually exclusive. The target vector ttt can have more than a positive class, so it will be a vector of 000s and 111s with CCC dimensionality where 000 is negative and 111 is positive class. One intutive way to understand multi-label classification is to treat multi-label classification as CCC different binary and independent classification problem where each output neuron decides if a sample belongs to a class or not. . Output Activation Functions . These functions are transformations applied to vectors coming out from the deep learning models before the loss computation. The outputs after transformations represents probabilities of belonging to either one or more classes based on multi-class or multi-label setting. . Sigmoid It squashes a vector in the range (0,1)(0,1)(0,1). It is applied independently to each element of vector sss. . f(si)=11+e−sif(s_i) = frac{1}{1 + e^{-s_{i}}}f(si​)=1+e−si​1​ . Softmax It squashes a vector in the range (0,1)(0, 1)(0,1) and all the resulting elements add up to 111. It is applied to the output vector sss. The Softmax activation cannot be applied independently to each element of vector s, since it depends on all elements of sss. For a given class sis_isi​, the Softmax function can be computed as: . f(s)i=e(si)∑jCesjf(s)_i = frac{e^{(s_i)}}{ sum_{j}^C e^{s_j}}f(s)i​=∑jC​esj​e(si​)​ . Losses . Cross Entropy Loss . The cross-entropy loss is defined as: . CE=−∑iCtilog(si)CE = - sum_i^C t_i log(s_i )CE=−i∑C​ti​log(si​) . where tit_iti​ and sis_isi​ are the goundtruth and output score for each class i in C. . Taking a very rudimentary example, consider the target(groundtruth) vector t and output score vector s as below: . Target Vector: [0.6 0.3 0.1] Score Vector: [0.2 0.3 0.5] . Then CE will be computed as follows: . CE = -(0.6)log(0.2) - 0.3log(0.3) - 0.1log(0.5) = 0.606 . . In supervised machine learning settings, elements of target vectors are either 1 or 0. The above example shows how CE is computed and how it is also applicable to find loss between the distributions. Categorical Cross-Entropy Loss . In multi-class setting, target vector t is one-hot encoded vector with only one positive class (i.e.ti=1t_i = 1ti​=1) and rest are negative class (i.e. ti=0t_i = 0ti​=0). Due to this, we can notice that losses for negative classes are always zero. Hence, it does not make much sense to calculate loss for every class. Whenever our target (ground truth) vector is one-hot vector, we can ignore other labels and utilize only on the hot class for computing cross-entropy loss. So, Cross-Entropy loss becomes: . CE=−log(si)CE = -log(s_i)CE=−log(si​) . . The above form of cross-entropy is called as Categorical Cross-Entropy loss. In multi-class classification, this form is often used for simplicity. The Categorical Cross-Entropy loss is computed as follows: . f(s)i=esi∑jCesj⇒CE=−∑iCtilog(f(s)i)⇒CE=−log(f(s)i)f(s)_i = frac{e^{s_i}}{ sum_{j}^{C}e^{s_j}} Rightarrow CE = - sum_i^C t_i log(f(s)_i) Rightarrow CE = -log(f(s)_i)f(s)i​=∑jC​esj​esi​​⇒CE=−i∑C​ti​log(f(s)i​)⇒CE=−log(f(s)i​) . As, SoftMax activation function is used, many deep learning frameworks and papers often called it as SoftMax Loss as well. . Binary Cross-Entropy Loss . Based on another classification setting, another variant of Cross-Entropy loss exists called as Binary Cross-Entropy Loss(BCE) that is employed during binary classification (C=2)(C = 2)(C=2). Binary classification is multi-class classification with only 2 classes. To dumb it down further, if one class is a negative class automatically the other class becomes positive class. In this classification, the output is not a vector s but just a single value. Let’s understand it further. . The target(ground truth) vector for a random sample contains only one element with value of either 1 or 0. Here, 1 and 0 represents two different classes (C=2)(C = 2)(C=2). The output score value ranges from 0 to 1. If this value is closer to 1 then class 1 is being predicted and if it is closer to 0, class 0 is being predicted. . BCE=−∑i=1C=2tilog(f(s)i)=−t1log(f(s1))−(1−t1)log(1−f(s1))BCE = - sum_{i=1}^{C=2}t_ilog(f(s)_i) = -t_1log(f(s_1)) - (1-t_1)log(1-f(s_1))BCE=−i=1∑C=2​ti​log(f(s)i​)=−t1​log(f(s1​))−(1−t1​)log(1−f(s1​)) . s1s_1s1​ and t1t_1t1​ are the score and groundtruth label for the class CiC_iCi​ in CCC. s2=1−s1s_2 = 1 -s_1s2​=1−s1​ and t2=1−t1t_2 = 1 - t_1t2​=1−t1​ are the score and groundtruth label for the class C2C_2C2​. If t1=0t_1 = 0t1​=0 then −t1log(f(s1))-t_1log(f(s_1))−t1​log(f(s1​)) would become 000 and (1−t1)log(1−f(s1))(1-t_1)log(1-f(s_1))(1−t1​)log(1−f(s1​)) would become active. Similarly, if t1=1t_1 = 1t1​=1 then −t1log(f(s1))-t_1log(f(s_1))−t1​log(f(s1​)) would become active and (1−t1)log(1−f(s1))(1-t_1)log(1-f(s_1))(1−t1​)log(1−f(s1​)) would become 000. The loss can be expressed as: . CE={−log(f(s1))ift1=1−log(1−f(s1))ift1=0CE = begin{cases} -log(f(s_1)) &amp; if &amp; t_1 = 1 -log(1-f(s_1)) &amp; if &amp; t_1 = 0 end{cases}CE={−log(f(s1​))−log(1−f(s1​))​ifif​t1​=1t1​=0​ . To get the output score value between [0,1], sigmoid activation function is used. . . Due to the using of Sigmoid Activation function it is also called as Sigmoid-Cross Entropy Loss. . f(s1)=11+e−s1⇒CE=−t1log(f(s1))−(1−t1)log(1−f(s1))f(s_1) = frac{1}{1+e^{-s_1}} Rightarrow CE = -t_1log(f(s_1)) - (1-t_1)log(1-f(s_1))f(s1​)=1+e−s1​1​⇒CE=−t1​log(f(s1​))−(1−t1​)log(1−f(s1​)) . Cross-Entropy in Multi-Label Classification . As described earlier, in multi-label classification each sample can belong to more than one class. With CCC different classes, multi-label classification is treated as CCC different independent binary classification. Multi-label classification is a binary classification problem w.r.t. every class. The output is vector sss consisting of CCC number of elements. Binary Cross-Entropy Loss is employed in Multi-Label classification and it is computed for each class in each sample. . Loss=∑i=1i=CBCE(ti,f(s)i)=∑i=1i=Ctilog(f(s)i)Loss = sum_{i=1}^{i=C}BCE(t_i, f(s)_i) = sum_{i=1}^{i=C}t_ilog(f(s)_i)Loss=i=1∑i=C​BCE(ti​,f(s)i​)=i=1∑i=C​ti​log(f(s)i​) . Focal Loss . Focal Loss was introduced in Focal Loss for Dense Object Detection paper by He et al (at FAIR). Object detection is one of the most widely studied topics in Computer Vision with a major challenge of detecting small size objects inside images. Object detection algorithms evaluate about 10410^4104 to 10510^5105 candidate locations per image but only a few locations contains objects and rest are just background objects. This leads to class imbalance problem. . Using Binary Cross-Entropy Loss for training with highly class imbalance setting doesn’t perform well. BCE needs the model to be confident about what it is predicting that makes the model learn negative class more easily they are heavily available in the dataset. In short, model learns nothing useful. This can be fixed by Focal Loss, as it makes easier for the model to predict things without being 80−10080-100%80−100 sure that this object is something. Focal Loss allows the model to take risk while making predictions which is highly important when dealing with highly imbalanced datasets. . . Though Focal Loss was introduced with object detection example in paper, Focal Loss is meant to be used when dealing with highly imbalanced datasets. How Focal Loss Works? . Focal Loss is am improved version of Cross-Entropy Loss that tries to handle the class imbalance problem by down-weighting easy negative class and focussing training on hard positive classes. In paper, Focal Loss is mathematically defined as: . FocalLoss=−αt(1−pt)γlog(pt)Focal Loss = - alpha_t(1 - p_t)^{ gamma}log(p_t)FocalLoss=−αt​(1−pt​)γlog(pt​) . . The above definition is Focal Loss for only one class. It has omitted the $$ sum$$ that would sum over all the classes $$C$$. To calculate total Focal Loss per sample, sum over all the classes. What is Alpha and Gamma ? . The only difference between original Cross-Entropy Loss and Focal Loss are these hyperparameters: alpha(α alphaα) and gamma(γ gammaγ). Important point to note is when γ=0 gamma = 0γ=0, Focal Loss becomes Cross-Entropy Loss. . Let’s understand the graph below which shows what influences hyperparameters α alphaα and γ gammaγ has on Focal Loss and in turn understand them. In the graph, “blue” line represents Cross-Entropy Loss. The X-axis or “probability of ground truth class” (let’s call it pt) is the probability that the model predicts for the ground truth object. As an example, let’s say the model predicts that something is a bike with probability 0.60.60.6 and it actually is a bike. In this case, pt is 0.60.60.6. In the case when object is not a bike, the pt is 0.4(1−0.6)0.4 (1-0.6)0.4(1−0.6). The Y-axis denotes the loss values at a given pt. . As can be seen from the image, when the model predicts the ground truth with a probability of 0.60.60.6, the Cross-Entropy Loss is somewhere around 0.50.50.5. Therefore, to reduce the loss, the model would have to predict the ground truth class with a much higher probability. In other words, Cross-Entropy Loss asks the model to be very confident about the ground truth prediction. . This in turn can actually impact the performance negatively: . The deep learning model can actually become overconfident and therefore, the model wouldn’t generalize well. . Focal Loss helps here. As can be seen from the graph, Focal Loss with γ&gt;1 gamma &gt; 1γ&gt;1 reduces the loss for “well-classified examples” or examples when the model predicts the right thing with probability &gt;0.5&gt; 0.5&gt;0.5 whereas, it increases loss for “hard-to-classify examples” when the model predicts with probability &lt;0.5&lt;0.5&lt;0.5. Therefore, it turns the model’s attention towards the rare class in case of class imbalance. . γ gammaγ controls the shape of curve. The higher the value of γ gammaγ, the lower the loss for well-classified examples, so we could turn the attention of the model towards ‘hard-to-classify’ examples. Having higher γ gammaγ extends the range in which an example receives low loss. . Another way, apart from Focal Loss, to deal with class imbalance is to introduce weights. Give high weights to the rare class and small weights to the common classes. These weights are referred as α alphaα. . But Focal Loss paper notably states that adding different weights to different classes to balance the class imbalance is not enough. We also need to reduce the loss of easily-classified examples to avoid them dominating the training. To deal with this, multiplicative factor (1−pt)γ(1-p_t)^{ gamma}(1−pt​)γ is added to Cross-Entropy Loss which gives the Focal Loss. . Focal Loss: Code Implementation . Here is the implementation of Focal Loss in PyTorch: . class WeightedFocalLoss(nn.Module): def __init__(self, batch_size, alpha=0.25, gamma=2): super(WeightedFocalLoss, self).__init__() self.alpha = alpha.repeat(batch_size, 1) self.gamma = gamma def forward(self, inputs, targets): BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=&#39;none&#39;) targets = targets.type(torch.long) at = self.alpha pt = torch.exp(-BCE_loss) F_loss = at*(1-pt)**self.gamma * BCE_loss return F_loss.mean() . References . Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names | What is Focal Loss and when should you use it? | A Beginner’s Guide to Focal Loss in Object Detection! |",
            "url": "https://theguywithblacktie.github.io/kernel/machine%20learning/pytorch/2021/05/20/cross-entropy-loss.html",
            "relUrl": "/machine%20learning/pytorch/2021/05/20/cross-entropy-loss.html",
            "date": " • May 20, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Adding Variable Number of Layers in Neural Network",
            "content": "Consider following code block that defines a fixed 2-layer neural network. Imagine a scenario, where the network has huge number of layers, and typing out each layer manually is just not feasible. An even more notable scenario is when the number of layers of network are not fixed and it depends on some other conigurations. This article deals with these scenarios and lays out solution. . class Net(nn.Module): def __init__(self, input_dim, output_dim): super(Net, self).__init__() linear_layer_one = nn.Linear(input_dim, output_dim) linear_layer_two = nn.Linear(input_dim, output_dim) def forward(self, input): output = linear_layer_two( linear_layer_one(input) ) return output . Recently, I was implementing a library related to Graph Networks in PyTorch framework. There I encountered the second scenario where the number of layers in the neural network were not fixed and it network required number of layers to be added as an input. The user of the library would specify how many layers is required as the input and that many layers would be added in the neural network. . As a daily user of Python, my first solution was to use a list data structure with a for loop to add n-number of layers, like below code block. . class Net(nn.Module): def __init__(self, input_dim, output_dim, nos_linear_layer): super(Net, self).__init__() self.nn_layers = [] for i in range(0,nos_linear_layer): linear_layer = nn.Linear(input_dim, output_dim) self.nn_layers.append(linear_layer) def forward(self, input): outputs = None for i,layer in enumerate(self.nn_layers): outputs = layer(input) outputs = torch.nn.functional.Softmax(outputs, 1) return outputs . Above code would look correct and would be expected to run without any issue. But, the main issue is that the linear layers stored in Python list would not be trained. On calling model.parameters(), PyTorch would simply ignore the parameters of linear layers stored in the Python list. . The correct way is to use PyTorch’s list nn.ModuleList. . class Net(nn.Module): def __init__(self, input_dim, output_dim, nos_linear_layer): super(Net, self).__init__() self.nn_layers = nn.ModuleList() for i in range(0,nos_linear_layer): linear_layer = nn.Linear(input_dim, output_dim) self.nn_layers.append(linear_layer) def forward(self, input): outputs = None for i,layer in enumerate(self.nn_layers): outputs = layer(input) outputs = torch.nn.functional.Softmax(outputs, 1) return outputs .",
            "url": "https://theguywithblacktie.github.io/kernel/pytorch/2021/05/14/vary-layers-pytorch.html",
            "relUrl": "/pytorch/2021/05/14/vary-layers-pytorch.html",
            "date": " • May 14, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://theguywithblacktie.github.io/kernel/2020/01/14/test-markdown-post.html",
            "relUrl": "/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is Ashish! While working in software engineering for 3 years, I pondered upon Machine Learning course by Andrew Ng and since then my daily learnings have never been the same. Machine Learning or AI (as many calls it for sophistication) is a vast field that is growing exponentially even today. Hence, I am documenting my learning notes in this blog to help out people and me in future looking for answers. Reach out to me on my social handles or via email incase you have any questions or find errors. Thanks :) . If you like any of my blog or found it helpful in your ML journey, do let me know by messaging me on social media. Otherwise you could also share your appreciation by sharing the work on social media and taging me along with it. These would really encourage me to write even more quality blogs. .",
          "url": "https://theguywithblacktie.github.io/kernel/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://theguywithblacktie.github.io/kernel/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}