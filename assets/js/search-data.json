{
  
    
        "post0": {
            "title": "Generative Adversarial Networks",
            "content": "We have come across various articles and posts about AI being capable of producing human like speech or generating images of non-existing people that are difficult to distinguish from real-life existence. These AI systems are built upon generative adversarial networks (GANs) - which Facebook AI research director Yann LeCun called “most interesting idea in last 10 years in ML” GANs were introduced in a paper by Ian Goodfellow and other researchers at the University of Montreal, including Yoshua Bengio, in 2014. . GANs’ potential for both good and evil is huge, because they can learn to mimic any distribution of data. That is, GANs can be taught to create worlds eerily similar to our own in any domain: images, music, speech, prose. They are robot artists in a sense, and their output is impressive - poignant even. But they can also be used to generate fake media content and are the technology underpinning Deepfakes. . To understand GANs, we need to understand generative algorithms and its counterpart discriminative algorithms. . Background: Generative Models v/s Discriminative Models . Discriminative Models . Discriminative models classifies the input data; i.e. given the features of an instance of data, they predict the label or cateogry to which the data belongs. For example, given , all the words in an email (this is data instance), a discriminative model would predict whether the email is spam or not_spam. spam is one of the labels, and the bag of words gathered from the email are the features that constitute the input data. When this problem is expressed mathematically, the label is called as $Y$ and the features are called as $X$. The formualtion $P(Y|X)$ is used to mean “the probability of y given x” which in this case would translate to “the probability that an email is spam given the words it contains”. . So discriminative models maps features to labels. They are concerned solely with that correlation. Discriminative models directly learns the conditional distribution $P(Y|X)$. . Generative Models . On the other hand, generative models learns the joint probability distribution $P(X,Y)$ by explicitly learning $P(X|Y)$ and $P(Y)$, and then uses it to compute $P(Y|X)$ through Bayes rule. . Given a model of the joint distribution, $P(X,Y)$, marginal distribution of $X$ &amp; $Y$ can be calculated as: . P(X)=∑yP(X,Y=y)  andP(Y)=∫xP(Y,X=x)P(X) = sum_{y}P(X, Y = y) space space and P(Y) = int_{x}P(Y, X = x)P(X)=y∑​P(X,Y=y)  andP(Y)=∫x​P(Y,X=x) . considering $X$ as continuous, hence integrating over it and $Y$ as discrete hence summing over it. . Either conditional distribution can be computed as: . P(Y∣X)=P(X,Y)P(X)P(Y|X) = frac{P(X,Y)}{P(X)}P(Y∣X)=P(X)P(X,Y)​ . Thats how, we can compute $P(Y|X)$ in generative models. . One way think about generative models from GANs perspective is that they do the opposite of discriminative models. Instead of predicting labels given the features, generative models attempt to predict features given a certain label i.e. $P(X|Y)$. Understanding it from the email example, generative model tries to answer this: Assuming the given eamil is spam, how likely are these features? While discriminative models care about relation between $Y$ and $X$, generative models care about “how you get $X$?” . Formal Definitions: . A discriminative model learns a function that maps the input data X to some desired output class label Y. In probabilistic terms, they directly learn the conditional distribution P(Y|X). | A generative model tries to learn joint probability of the input data and labels simultaneously, i.e. P(X,Y). This is converted to P(X|Y) for classification via Bayes rule, but the generative ability could be used for something else as well, such as creating new (X,Y) sample. | . The word &quot;generative&quot; in generative models does not mean that the model generates actual new data additionally to the dataset. It refers to the nature of the theoretical model, in the sense that the generative approach assumes that any sample of data in generated from some distribution, and it tries to estimate this distribution. Once the distribution is estimated the model could be used to actually generate instances following this distribution. . Both these models are used in supervised learning where one wants to learn a rule that maps input $X$ to output $Y$. Some argues that the discriminative models are better as they directly models the quantity we care about i.e. $Y$ and hence no efforts are spent on modelling the input $X$. However, generative models has its own advantages such as capability of dealing with missing data. Since, generative models concerns about $P(X,Y)$ and $P(X)$ at the same time in order to predict $P(Y|X)$, they have less degree of freedom as compared to discriminative models. So generative models are more robust, less prone to overfitting than discriminative models. . Generative Models are hard . As explained earlier, generative models are better than discriminative models in certain aspects but they are even difficult to train. Generative models tackle a more difficult task than analogous discriminative models. Generative models have to model more. . A generative model for images might capture correlations like “things that look like boats are probably going to appear near things that look like water” and “eyes are unlikely to appear on foreheads”. These are very complicated distributions. . In contrast, a discriminative model might learn the difference between “sailboat” or “not sailboat” by just looking for a few tell-tale patterns. It could ignore many of the correlations that the generative models must get right. . Discriminative models try to draw boundaries in the data space, while generative models try to model how data is placed throughut the space. For example, the following diagrams shows discriminative and generative models of handwritten digits: . . The discriminative model tries to tell the difference between handwritten 0’s and 1’s by drawing a line in the data space. If it gets the line right, it can distinguish 0’s from 1’s without ever having to model exactly where the instances are placed in the data space on either side of the line. . In contrast, the generative model tries to produce convincing 1’s and 0’s by generating digits that fall close to their real counterparts in the data space. It has to model the distribution throughout the data space. GANs offer an effective way to train such rich models to resemble a real distribution. . Generative Adversarial Networks . Lets dive into GANs and their working. . The main idea behind GANs is to have two competing neural network models. One takes noise as input and generates samples (hence called generator) wheras other model (called discriminator) receives samples from both the generator and the training data, and has to be able to distinguish between the two inputs. These two networks play a continuous game, where the generator is leaning to produce more and more realistic samples, and the discriminator is learning to get better and better at distinguishing generated data from the real data. . . Lets take an example. Lets say we are going to generate hand-written numerals like those found in the MNIST dataset, which is taken from the real world. The goal of the discriminator here, when shown an instance from the true MNIST dataset, is to recognize those that are authentic and belong to MNIST dataset. . Meanwhile, the generator is creating new, synthetic images just like MNIST dataset and passes it to the discriminator. Generator does so in the hope that its generated synthetic images will be deemed authentic by the discriminator even though they are fake. The goal of the generator is to generate hand-written digits not to be caught by discriminator for being fake. The goal of the discriminator is to identify images coming from the generators as fake. . . Here are the steps a GAN takes: . The generator takes in random numbers and returns an image | This generated image is fed into the discriminator alongside a stream of images taken from the actual, ground-truth dataset. | The discriminator takes in both real and fake images and return probabilities for whether image is fake or real. | . . Discriminator takes real images and fake images one after another (not simulataneously) and calculate their probabilites for being fake or real So there is a double feedback loop (or two ways through which weights are updated): . The discriminator is in a feedback loop with the ground truth of the real images i.e. discriminator’s weights are updated based on its performance to detect real images as real | The generator is in a feedback loop with the discriminator i.e. generator’s weights are updated based on discriminator’s performance to detect generated images as fake | . The analogy that is often used to understand GAN is that the generator is like a forger trying to produce some counterfeit material, and the discriminator is like the police trying to detect the forged items. This setup may also seem somewhat reminiscent of reinforcement learning, where the generator is receiving a reward signal from the discriminator letting it know whether the generated data is accurate or not. The key difference with GANs however is that we can backpropagate gradients from the discriminator network back to the generator network, so the generator knows how to adapt its parameters in order to produce output data that can fool discriminator. Lets learn how loss is propagated from one discriminator network to another generator network. . Discriminator . The discriminator in GANs are simply a classifier. It tries to distinguish real data from the data generated by the generator. It could use any network architecture appropriate to the type of data it’s classifying. In MNIST example above, the discriminator network could be a standard convolutional network that can categorize the images fed to it; a binary classifier labelling images as real or fake. . As explained earlier, discriminator’s training data comes from two sources: . Real data instances taken from the real world. The discriminator uses these instances as positive examples during training. | Fake data instances created by the generator. The discriminator uses these instances as negative examples during training. | . The discriminator connects to two loss functions. During discriminator training, the discriminator ignores the generator loss and just uses the discriminator loss. During discriminator training: . The discriminator classifies both real data and fake data (from the generator). | The discriminator loss penalizes the discriminator for misclassifying a real instance as fake or a fake instance as real. | The discriminator updates it weights through backpropagation from the discriminator loss through discriminator network. | . Generator . The generator part of GAN learns to create fake data by incorporating feedback from the discriminator. It learns to make the discriminator classify its output as real. . GAN takes random noise as its input. The generator then transforms this noise into a meaningful output. By introducing noise, we can get the GAN to produce a wide variety of data, sampling from different places in the target distribution. Experiments suggest that the distribution of noise doesn’t matter much, so we can choose something that’s easy to sample from, like a normal distribution. For convenience, the space from which the noise is sampled is usually of smaller dimension than the dimensionality of the output space. . A neural net is trained by altering its weights to reduce the error. In GANs, however, the generator is not directly connected to the loss that we’re trying to affect. The generator feeds into the discriminator net, and the discriminator produces the output through which loss is computed. Backpropagation adjusts weights in right direction by calculating the weight’s impact on the output. But the impact of a generators weight depends on the impact of the discriminator weight it feeds into. impact So this extra chunk of discriminator network is included in the backpropagation that trains the generator. Backpropagation starts at the output of GAN and flows back through the discriminator into the generator. . At the same time, we don’t want the discriminator weights to be altering during the generator training. . So generator is trained in following way: . Sample random noise | Produce generator output from sampled random noise | Get discriminator “Real” or “Fake” classification for generator output | Calculate loss from discriminator classification | Backpropagate through both the discriminator and generator to obtain gradients. | Use gradients to change only the generator weights. | . GAN Training . We saw, how individually generator and discriminator train in their specific sections above. GAN as a whole is trained when both its generator and discriminator network get trains. GAN’s training proceeds in a alternating way: . The discriminator trains for one or more epochs. | The generator trains for one or more epochs. | Steps 1 and 2 are repeated to continue train the generator and discriminator. | When discriminator network is trained, generator network is kept constant. Similarly, discriminator network is kept constant when generator network is trained. Its this back and forth of training that allows GANs to tackle intractable generative problem. . . As the generator improves with training, the discriminator performance gets worse which hampers generator&#39;s training As the generator improves with training, the discriminator performance gets worse since the discriminator can’t easily tell the difference between real and fake. If the generator succeeds perfectly, then the discriminator has a 50% accuracy. This progression poses a problem for GAN as a whole: the discriminator feedback to generator gets less meaningful overtime. If the GAN continues training past the point when the discriminator is giving completely random feedback, then the generator starts to train on junk feedback, and its quality may collapse. . GAN Mathematics . Let us understand how GAN is trained mathematically. . We learnt in earlier sections how GAN can be seen as an interplay between two different models i.e. generator and discriminator. As a result, each of the model would have its own loss function. Lets define the loss function for each of the two models but before that first some notations for loss functions. . Notations and Loss Functions . x:  Real  data  samplez:  Latent  Vector  from  generator  modelG(z):  Fake  data  from  generatorD(x):  Discriminator′s  evaluation  of  real  dataD(G(z)):  Discriminator′s  evaluation  of  fake  dataError(a,b):  Error  between  ′a′  and  ′b′x : space space Real space space data space space sample z : space space Latent space space Vector space space from space space generator space space model G(z) : space space Fake space space data space space from space space generator D(x) : space space Discriminator&amp;#x27;s space space evaluation space space of space space real space space data D(G(z)) : space space Discriminator&amp;#x27;s space space evaluation space space of space space fake space space data Error(a,b): space space Error space space between space space &amp;#x27;a&amp;#x27; space space and space space &amp;#x27;b&amp;#x27; x:  Real  data  samplez:  Latent  Vector  from  generator  modelG(z):  Fake  data  from  generatorD(x):  Discriminator′s  evaluation  of  real  dataD(G(z)):  Discriminator′s  evaluation  of  fake  dataError(a,b):  Error  between  ′a′  and  ′b′ . Discriminator Loss Function The goal of the discriminator is to correctly label generated images as false($0$) and real data samples as true($1$). Therefore, the loss function of discriminator would be something like this: . LD=Error(D(x),1)+Error(D(G(z)),0)(1)L_{D} = Error(D(x),1) + Error(D(G(z)), 0) tag{1}LD​=Error(D(x),1)+Error(D(G(z)),0)(1) . Here, $Error$ is being referring to some function that tells us the distance or difference between two functional parameters. . Generator Loss Function The goal of the generator is to confuse the discriminator as much as possible such that it mislabels the generated images as being true. The loss function of generator could be defined as: . LG=Error(D(G(z)),1)(2)L_{G} = Error(D(G(z)), 1) tag{2}LG​=Error(D(G(z)),1)(2) . We have to remember that a loss function is need to be minimized. Hence, in the case of generator, loss function would be minimized such that the difference between 1 (the label for real data) and discriminator’s evaluation of generated data is minimum (i.e. optimally it should be 0). . BCE in GANs . A common loss function that is used in binary classification tasks is binary cross entropy. As a quick review, let’s see how cross entropy looks like: . CE(p,q)=−∑x∈χp(x)logq(x)CE(p,q) = - sum_{x in chi}p(x)logq(x)CE(p,q)=−∑x∈χ​p(x)logq(x) where $p$ is the predicted probability and $q$ is the ground truth label of data $x$ belonging to $ chi$ dataset. $CE(p,q)$ could be further simplied for binary classification problems (i.e. only two labels: $0$ and $1$) and called as binary cross entropy (BCE): . BCE(y,y^)=−∑x=1χ=2ylog(y^)+(1−y)log(1−y^)BCE(y, hat{y}) = - sum_{x=1}^{ chi = 2}ylog( hat{y}) + (1-y)log(1- hat{y})BCE(y,y^​)=−x=1∑χ=2​ylog(y^​)+(1−y)log(1−y^​) . More detail on Cross Entropy and its variants can be read here. . The $Error$ written above in $(1)$ and $(2)$ is this BCE. Binary Cross Entropy fulfills the objective of measuring how different two distributions are in the context of binary classification of determining whether an input data point is true or false. Applying BCE to loss function in $(1)$: . LD=−∑x∈χ,z∈ζlog(D(x))+log(1−D(G(z)))(3)L_{D} = - sum_{x in chi, z in zeta}log(D(x)) + log(1 - D(G(z))) tag{3}LD​=−x∈χ,z∈ζ∑​log(D(x))+log(1−D(G(z)))(3) . Same can be done for $(2)$ as well: . LG=−∑z∈ζlog(D(G(z)))(4)L_{G} = - sum_{z in zeta}log(D(G(z))) tag{4}LG​=−z∈ζ∑​log(D(G(z)))(4) . Now there are two loss functions which would be minimized to train the generator and discriminator. For the generator loss function to be small, $D(G(z))$ should be close to $1$, since $log(1) = 0$. . Minor Caveats from Original Paper . The original paper by Ian Goodfellow presents a slightly different version of the two loss functions derived above: . maxD{log(D(x))+log(1−D(G(z)))}(5) underset{D}{max} {log(D(x)) + log(1- D(G(z))) } tag{5}Dmax​{log(D(x))+log(1−D(G(z)))}(5) . The above loss function is of generator’s and on looking closely it is very alike to equation $(1)$. The only difference between equation $(1)$ and $(4)$ is the sign and whether the respective equation is need to be minimize or maximize. In $(1)$, the loss function is framed to be minimized, whereas the original formulation frames the loss function to be maximized with the sign flipped. Hence, we get equation $(5)$ as the loss function for discriminator to be maximized. . The generator is competing against the discriminator striving to make discriminator think generated data as real. So, it will try to minimize the equation $(5)$ and generator’s loss function would be: . minG{log(D(x))+log(1−D(G(z)))}(6) underset{G}{min} {log(D(x)) + log(1- D(G(z))) } tag{6}Gmin​{log(D(x))+log(1−D(G(z)))}(6) . In the oiginal paper, these loss functions are combined to get the following equation: . L=minG maxD{log(D(x))+log(1−D(G(z)))}(7)L = underset{G}{min} space underset{D}{max} {log(D(x)) + log(1- D(G(z))) } tag{7}L=Gmin​ Dmax​{log(D(x))+log(1−D(G(z)))}(7) . Model Optimization . Now that loss functions for generator and discriminator has been defined, it’s time to leverage mathematics to solve the optimization problem, i.e. finding the parameters for the generator and discriminator such that the loss functions are optimized. . Training the Discriminator . When training a GAN, one model is train at a time. In other words, when training the discriminator, the generator is fixed. In the min-max loss function, the quantity of interest can be defined as a function of $G$ and $D$. Let’s call this the value function: . V(G,D)=Ex∼pdata[log(D(x))]+Ez∼pz[log(1−D(G(z)))]V(G,D) = mathbb{E}_{x sim p_{data}}[log(D(x))] + mathbb{E}_{z sim p_{z}}[log(1-D(G(z)))]V(G,D)=Ex∼pdata​​[log(D(x))]+Ez∼pz​​[log(1−D(G(z)))] . Let’s create a new variable, $y = G(z)$, and use this substitution to rewrite the value function: . V(G,D)=Ex∼pdata[log(D(x))]+Ey∼pg[log(1−D(y))]V(G,D)=∫x∈χpdata(x)log(D(x))+pg(x)log(1−D(x))dx(8)V(G,D) = mathbb{E}_{x sim p_{data}}[log(D(x))] + mathbb{E}_{y sim p_{g}}[log(1 - D(y))] V(G,D) = int_{x in chi}p_{data}(x)log(D(x)) + p_{g}(x)log(1-D(x))dx tag{8}V(G,D)=Ex∼pdata​​[log(D(x))]+Ey∼pg​​[log(1−D(y))]V(G,D)=∫x∈χ​pdata​(x)log(D(x))+pg​(x)log(1−D(x))dx(8) . The goal of the discriminator is to maximize this value function. Through a partial derivative of $V(G,D)$ with respect to $D(x)$, we see that the optimal discriminator, denoted as $D^{*}(x)$, occurs when . pdata(x)D(x)−pg(x)1−D(x)=0 frac{p_{data}(x)}{D(x)} - frac{p_{g}(x)}{1-D(x)} = 0D(x)pdata​(x)​−1−D(x)pg​(x)​=0 . Rearranging it, we get . D∗(x)=pdata(x)pdata(x)+pg(x)(9)D^{*}(x) = frac{p_{data}(x)} {p_{data}(x) + p_{g}(x)} tag{9}D∗(x)=pdata​(x)+pg​(x)pdata​(x)​(9) . And this is the condition for the optimal discriminator!. Note that the formula makes intuitive sense: if some sample $x$ is highly genuine, $p_{data}(x)$ is expected to be close to one and $p_{g}(x)$ to be converge to zero, in which case the optimal discriminator would assign 1 to that sample. On the other hand, for a generated sample $x=G(z)$, optimal discriminator would assign a label of zero, since $p_{data}(G(z))$ should be close to zero. . Training the Generator . To train the generator, the discriminator is assumed to be fixed and lets proceed with the analysis of the value function. Plugging the result obtained in $(9)$ in value function: . V(G, D^{}) = mathbb{E}_{x sim p_{data}}[log(D^{}(x))] + mathbb{E}{z sim p{g}}[log(1-log(D^{*}(x)))] &amp;= mathbb{E}{x sim p{data}}[log frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)}] + mathbb{E}{z sim p{g}}[log frac{p_{g}(x)}{p_{data}(x)+p_{g}(x)}] tag{10} . The min-max fomrulation is a concise one-liner that intuitively demonstrates the adversarial nature of the competition between the generator and the discriminator. However, in practise, separate loss functions are defined for generator and discriminator. The minimax version is simple and allows for easier theoretical results, but in practice its not that useful, due to gradient saturation. As Ian Goodfellow himself notes: . In the minimax game, the discriminator minimizes a cross-entropy, but the generator maximizes the same cross-entropy. This is unfortunate for the generator, because when the discriminator successfully rejects generator samples with high confidence, the generator&#39;s gradient vanishes. . space space space space space space space space space space space space space space space space space space space . Refereces . A Beginner’s Guide to Generatvive Adversarial Networks (GANs) | Stackoverflow | Generative Adversarial Networks - Google Developers | GAN Mathematics | More to Read . Stackverflow: Generative v/s Discriminative | This is because the gradient of the function $y=log(x)$ is steeper near $x=0$ than that of the function $y=log(1-x)$, meaning when trying to maximize $log(D(G(z)))$, or equivalently minimizing $-log(D(G(z)))$ is going to lead to quicker, more substantial improvements to the performance of the generator that trying to minimize $log(1-D(G(z)))$. .",
            "url": "https://theguywithblacktie.github.io/kernel/2021/06/30/GAN.html",
            "relUrl": "/2021/06/30/GAN.html",
            "date": " • Jun 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Understanding Support Vector Machine",
            "content": "In Machine Learning, out of the many available supervised classification algorithms, Support Vector Machine (SVM) is one of the easiest algorithm and yet sometimes it becomes difficult to grasp due to its little nuances. In this blog, I try to lay down my notes on unboxing the SVM blackbox and make it easy to understand in detail. This blog focusses on entirety of SVM, from its introduction to classification and kernel tricks. . The idea of SVM is simple: The algorithm creates a line or a hyperplane that separates data into different classes. Let’s understand how SVM creates the hyperplane and what are its other jargon. . What is SVM? . Support Vector Machine (SVM) is a supervised ML algorithm that can be used for classification and regression as well. However, it is mostly used in classification problems. . In SVM classificatio, each data point is plotted in an n-dimensional space (n = Nos. of features). and classified by finding the best hyperplane that differentiates the classes well. . In the above image, data points are plotted in a 2-dimensional space and solid black line is the hyperplane that is able to divide the dataset in two classes viz. orange and green color. The hyperplane becomes the decision boundary where if datapoints are above the hyperplane( black line) they belong to orange class and if datapoints are below the hyperplane they belong to green class. . . Classification is not done based on hyperplane decision boundary alone but based on hyperplane plus support vector decision region which is explained in next section. . Hyperplanes segregates the datasets into two classes only. Hyperplanes are n-1 -dimensional planes where n is total number of features of the dataset. From the above image, data is 2 dimensional and hyperplane divding the space is a 1 dimensional plane. . Taking it further: in the above example there is a hyperplane drew that differentiates the dataset in two classes very well. But on keenly thinking, there are other possible hyperplanes too which could be selected that divides the dataset. Something like in the image below: . Apart from the black coloured hyperplane line, there are other infinite hyperplanes available(to name a few: red, blue, yellow) which could differentiate the dataset too. But which is the best hyperplane among all and what is the criteria to call a hyperplane best? This question is answered with the help of Support Vectors. . What are Support Vectors? . Support Vectors are the vectors passing through the closest data points to the hyperplanes and are parallel to the hyperplane. The distance between the support vectors and hyperplane is called as margin. The thumb rule to identify the right hyperplane among infinitly available hyperplanes is: “Select the hyper-plane which segregates the two classes better such that the margin is maximum between the support vector and hyperplane”. . In above image, the middle solid black colored line is the hyperplane that divides the dataset into classes whereas two parallel dashed lines are its support vectors and they pass through the closest datapoints to the hyperplane. . The margin is maximum ever possible due to these chosen support vectors and hence the hyperplane derived from it is the best hyperplane to classify the datapoints into classes. . Few points on Support Vectors: . 1. The data/vector points closest to the hyperplane are known as **support vector** data points because only these two points are contributing to the result of SVM, other points are not. 2. If a data point is not an support vector, removing it has no effect on the model. 3. Deleting the support vector will change the position of the hyperplane. . . The larger the margin the better it is. Intuition for Large Margins . In SVM, the classification is not done based on the hyperplane decision boundary but even support vectors are also considered in classification. The region between two support vectors which includes hyperplane becomes the decision region. In fig. 3, if datapoints are above the decision region towards right they will be classified into orange class and if datapoints are below the decision region towards left they will be classified into green class. . Maximizing the margin seems good because data points near the decision region represent very uncertain classification decisions. For example, after training SVM we got a certain hyperplane with its support vectors such that the margin is very-very small. In this case SVM does not have confidence on its prediction of datapoints which are very close to the decsision region: there is almost a $50 %$ chance of the classifier deciding either way. Whereas a SVM classifier with a large margin makes no low certainty classification decisions. A large margin gives classifier safety margin where a slight error in measurement or a slight document variation will not cause a misclassification. . Mathematics for SVM . Lets go through the mathematics of SVM and understand SVM manages to select the hyperplane with maximum margin possible. . Generally, the equation of a line is given by: $y=ax+b$. We should define the hyperplane equation with this equation but however, we will define the equation of hyperplane in this form: $W^TX = 0$. Lets see how these two forms are related. . In hyperplane equation the names of the variables are in bold which means that that they are vectors and $W^TX$ represent the inner product(i.e. dot product) of two vectors. . Note that y=ax+by=ax+by=ax+b is same as y−ax−b=0y-ax-b=0y−ax−b=0. We can write: $W = begin{bmatrix}-b cr-a cr1 end{bmatrix}$ and $X = begin{bmatrix}1 cr x cr y end{bmatrix}$. Then: . WTX=−b×(1)+(−a)×x+1×yWTX=y−ax−bW^TX = -b times (1) + (-a) times x + 1 times y W^TX = y - ax -bWTX=−b×(1)+(−a)×x+1×yWTX=y−ax−b . The two equations are just different ways of expressing the same thing. It is interesting to note that wow_owo​ is −b-b−b which determines the intersection of the line with the vertical axis. . We use hyperplane equation $W^TX$ instead as $y=ax+b$ for two reasons: . it is easier to work in more than two dimensions with this notation, | the vector $W$ will always be normal to hyperplane ($W$ will always be normal because it is used to define the hyerplane, so by definition it will be normal. More Info Here) | Compute distance from a point to the hyperplane . In Fig. 4, there is a hyperplane that divides two sets of data. Here, there is no Y-intercept i.e. $w_0 = 0$. . . As can be seen from Fig. 4, the equation of hyperplane is: x2=−2x1x_2 = -2x_1x2​=−2x1​ which is equivalent to WTX=0W^TX = 0WTX=0 with $W = begin{bmatrix}2 cr1 end{bmatrix}$ and $X = begin{bmatrix}x_1 cr x_2 end{bmatrix}$. Note that vector $W$ is shown in the figure. . We would like to compute the distance between point $A(3,4)$ and the hyperplane. This is the distance between $A$ and its projection onto the hyperplane. . . We can view point $A$ as a vector from the origin to $A$. If we project this vector $A$ onto $W$ we get vector $P$. The distance between point $A(3,4)$ and hyperplane is same as $||p||$. We will be finding $||p||$. . Let $u$ denote the direction of vector $W$ which is given by $u = frac{W}{||w||}$. . ∥w∥=22+12=5 |w | = sqrt{2^2 + 1^2} = sqrt{5}∥w∥=22+12​=5​. So, $u$ becomes $u = begin{bmatrix} frac{2}{ sqrt{5}}, frac{1}{ sqrt{5}} end{bmatrix}$. . To compute $||p||$, we need to find value of vector $P$. From, the figure we can conclude that $P$ is the orthogonal projection of $A$ onto $W$. . The dot product between $A$ and $W$ is: $A.W = ||a|| times ||w|| cos theta Longrightarrow cos theta = frac{A.W}{||a|| times ||w||}$ . If we apply, Pythagoras theorem in the above figure we will also get: $ cos theta = frac{||p||}{||a||}$. . Equating above two equations: . A.W∥a∥×∥w∥=∥p∥∥a∥ frac{A.W}{ |a | times |w |} = frac{ |p |}{ |a |}∥a∥×∥w∥A.W​=∥a∥∥p∥​ . A.W∥w∥=∥p∥ frac{A.W}{ |w |} = |p | ∥w∥A.W​=∥p∥ . We already know that: . u=W∥w∥u = frac{W}{ |w |}u=∥w∥W​ . Substituting $u$ in tha above equation . ∥p∥=u.A |p | = u . A∥p∥=u.A . Vector $P$ is in same direction as vector $W$. Hence, we can define $u$ in terms of vector $p$ as well: . u=P∥p∥⟹P=∥p∥×uu = frac{P}{ |p |} Longrightarrow P = |p | times uu=∥p∥P​⟹P=∥p∥×u . P=(u.A)×uP = (u.A) times uP=(u.A)×u . So, . p=(3×25+4×15)up = (3 times frac{2}{ sqrt{5}} + 4 times frac{1}{ sqrt{5}} ) up=(3×5 . ​2​+4×5 . ​1​)u . p=105up = frac{10}{ sqrt{5}}up=5 . ​10​u . p=(105×25,105×15)p = ( frac{10}{ sqrt{5}} times frac{2}{ sqrt{5}} , frac{10}{ sqrt{5}} times frac{1}{ sqrt{5}})p=(5 . ​10​×5 . ​2​,5 . ​10​×5 . ​1​) . p=(4,2)p = (4, 2)p=(4,2) . ∥p∥=(42+22)=25 |p | = sqrt(4^2 + 2^2) = 2 sqrt{5}∥p∥=( . ​42+22)=25 . ​ . Compute the margin of hyperplane . Now that we have distance $||p||$ between $A$ and hyperplane, the margin is defined by: . margin=2×∥p∥=45margin = 2 times |p | = 4 sqrt{5}margin=2×∥p∥=45 . ​ . Finding the optimal-best hyperplane . In above section, we first considered a hyperplane that divided the dataset into two classes. We took closest point to this hyperplane i.e.point $A$, as one of the support vector will pass through it and calculated its ($||p||$) distance from the hyperplane. With $margin = 2 times ||p|$ we would get two support vectors and hyperplane as shown below: . . As we have seen earlier, the best hyperplane is the one with the maximum margin. In Fig. 4, the margin $M_1$ between two red support vectors is not the biggest margin possible. The biggest margin is margin $M_2$ with a different hyperplane which is the optimal one is shown in Fig. 5: . . Optimal hyperplane is slightly left of the initial hyperplane that we considered. It is found by simply tracing a line crossing $M_2$ in its middle. We can say that margins and hyperplanes are closely related and one can be found from another: . If we have hyperplane we can compute its margin with respect to some data point. If we have margin delimited by two support vectors, we can find the hyperplane passing right in the middle of the margin. . . Technically, support vectors are also hyperplanes. Finding the biggest margin is same as finding the best optimal hyperplane . Finding biggest margin . The biggest margin can be found in following steps: . 1. Take a dataset. 2. Select two hyperplanes(referring to support vectors here) which separate the data with no points between them 3. Maximize the distance (margin) . Step 1: Defining the dataset . Usually, the dataset consists of $n$ datapoints of $X_i$. Each $X_i$ is associated with a value $y_i$ inidicating the element belongs to the class($+1$) or not($-1$). . . In SVM, classes are represented by +1 and -1 rather than 1 and 0. . . Moreover, each vector $X_i$ consists of lot of dimensions. Lets say that $X_i$ is a $p$-dimensional vector. So dataset $D$ is the set of $n$ couples of element $(X_i, y_i)$. The more formal definition of dataset is: . D={(Xi,yi)∣Xi∈Rp,yi∈{−1,1}}i=1nD = {(X_i, y_i) | X_i in mathbb{R}^p, y_i in {-1, 1 } }_{i=1}^{n}D={(Xi​,yi​)∣Xi​∈Rp,yi​∈{−1,1}}i=1n​ . Step 2: Selecting two hyperplanes with no data points between them . We need to find two hyperplanes with the help of datapoints such that there are no datapoints between these two hyperplanes. These two hyperplanes are actually the support vectors and with the help of these we will derive the optimal hyperplane. . In the previous examples above, the datapoints were linearly separable and the hyperplane could be easily identified with the eye itself by visualisation. With dataset being non-linearly separable (like shown below) it becomes difficult to identify the hyperplane that divides the dataset. Moreover, whenever data is of $p$-dimension it becomes ever more difficult. . . So let’s assume dataset $D$ is linearly separable and we would want to find two hyperplanes with no points between them without visualisation. . We saw earlier, equation of hyperplane can be written as: $W^{T}X = 0$. We will change this equation notation to: . W.X+b=0W.X + b = 0W.X+b=0 . Not that in earlier hyperplane equation, vector $W$ contains $b$ but in new notation, $b$ has been taken out. $W^{T}X$ and $W.X$ both represent dot product. Both equations represent hyperplane and are same equations just that they are in different notations. The new equation has been used going forward. . Given a hyperplane $H_o$ separating the dataset and satisfying: . W.X+b=0W.X + b = 0W.X+b=0 . We can select two other hyperplanes $H_1$ and $H_2$ which also separates the data and have the following equations: . W.X+b=δW.X + b = deltaW.X+b=δ . and . W.X+b=−δW.X + b = - deltaW.X+b=−δ . so that $H_o$ is equidistant from $H_1$ and $H_2$. . Here, the variable $ delta$ is not necessary and so to simply we can set it to $1$. So, . W.X+b=1W.X+b=−1W.X + b = 1 W.X + b = -1W.X+b=1W.X+b=−1 . With the help of above hyperplane equations, we will select the hyperplanes which satisfies the following constraints: . For each vector $X_i$ either: . W.Xi+b≥1 for Xi having class 1W.Xi+b≤−1 for Xi having class −1W.X_i + b geq 1 space for space X_i space having space class space 1 W.X_i + b leq -1 space for space X_i space having space class space -1W.Xi​+b≥1 for Xi​ having class 1W.Xi​+b≤−1 for Xi​ having class −1 . Understanding the constraints . In the following figures, all the red data points belong to class 1 whereas all blue data points belong to class -1. . Consider we have two hyperplanes with equations $W.X_i + b = 1$ and $W.X_i + b = - 1$ and all the datapoints follows the constraints as described above. . Let’s look at Fig. 6 below and consider the point A. It is red so it belong to class 1 and we need to verify it does not violate constraint $W.X_i + b geq 1$. When $X_i = A$, we see that the point is on the hyperplane and so $W.X_i + b = 1$ and the constraint is respected. Same goes for $B$. When $X_i = C$, we see the point is above the hyperplane so $W.X_i + b ge 1$ and the constraint is respected. The same applied for $D, space E, space F$ and $G$. . . With an analogous reasoning we can see second constraint is respected for class -1. . There is another set of hyperplanes that respects the constraints: . . . In above two images, two sets of hyperplanes have different sets of values for W and b and hence they are two different set of hyperplanes respecting the constraints. Remember we get value for W and b by training with following the constraints Let’s see when constraints are not being respected. Below image shows sequences of instances where constraints are not followed: . . Whenever, the constraints are not being followed we cannot select those two hyperplanes. In every failed instances above, there are datapoints between the two sets of hyperplanes which we don’t want. . Let’s combine two constraints into one equation for simplicity. . Starting with equation (2) . for Xi having class −1W.Xi+b≤−1for space X_i space having space class space -1 W.X_i + b leq -1for Xi​ having class −1W.Xi​+b≤−1 . Multiply both sides with $y_i$ (which is $-1$ for this equation) . yi×(W.Xi+b)≥yi(−1)⟹yi×(W.Xi+b)≥1 for Xi having class −1            (3)y_i times (W.X_i + b) geq y_i(-1) Longrightarrow y_i times (W.X_i + b) geq 1 space for space X_i space having space class space -1 space space space space space space space space space space space space (3)yi​×(W.Xi​+b)≥yi​(−1)⟹yi​×(W.Xi​+b)≥1 for Xi​ having class −1            (3) . In equation (1), multiply both sides with $y_i$ and as $y_i =1$, sign of inequation doesn’t change . yi×(W.Xi+b)≥1for Xi having class 1                              (4)y_i times (W.X_i + b) geq 1 for space X_i space having space class space 1 space space space space space space space space space space space space space space space space space space space space space space space space space space space space space space (4)yi​×(W.Xi​+b)≥1for Xi​ having class 1                              (4) . Nw we can combine equations (3) and (4) and have one equation for both the constraints . yi×(W.Xi+b)≥1for all 1≤i≤n                           (5)y_i times (W.X_i + b) geq 1 for space all space 1 leq i leq n space space space space space space space space space space space space space space space space space space space space space space space space space space space (5)yi​×(W.Xi​+b)≥1for all 1≤i≤n                           (5) . We now have a unique constraint in equation (5) instead of two equations (1), (2). . Step 3: Maximize distance between two hyperplanes . Let’s find the distance between two hyperplanes first. . Let: . $H_o$ be the hyperplane having equation $W.X + b = -1$ | $H_1$ be the hyperplane having equation $W.X + b = 1$ | $x_o$ be a point on hyperplane $H_0$ | . We will call $m$ the perpendicular distance from $x_o$ to the hyperplane $H_1$. By definition, $m$ is the $margin$ and hence $m$ is the distance between hyperplane $H_o$ and $H_1$. . . To find $m$, we would find a vector such that: . has magnitude of $m$ | it is perpendicular to hyperplane $H_1$ | Fortunately, we already know a vector perpendicular to $H_1$, i.e. vector $W$ (because $H_1 = W.X + b = 1$) . . Lets define $u = frac{W}{||W||}$ as the unit vector of $W$. As its a unit vector, $||u|| = 1$ and it has the same direction as $W$, so it is also perpendicular to $H_1$. . . . We can&#39;t add scalar to vector but we can multiply scalar to a vector If we multiply $u$ by scalar $m$ we get the vector $k = m u $ and . $||k|| = m$ | $k$ is perpendicular to $H_1$ (because it has the same direction as $u$) | From these properties we can see that $k$ is the vector we are looking for: . . k=mu=mW∥W∥                           (6)k = mu = m frac{W}{ |W |} space space space space space space space space space space space space space space space space space space space space space space space space space space space (6)k=mu=m∥W∥W​                           (6) . If we start from the point $X_o$ and add $k$ we find that the point $Z_o = X_o + k$ is in the hyperplane $H_1$ as shown in below figure: . . The fact that $Z_o$ lies on $H_1$ means that: . W.Zo+b=1W. Z_o +b = 1W.Zo​+b=1 . W.(Xo+k)+b=1W.(X_o + k) + b = 1W.(Xo​+k)+b=1 . Replacing $k$ with equation (6): . W.(Xo+mW∥W∥)+b=1W.(X_o + m frac{W}{ |W |}) + b = 1W.(Xo​+m∥W∥W​)+b=1 . W.Xo+mW.W∥W∥+b=1W.X_o + m frac{W.W}{ |W |} + b = 1W.Xo​+m∥W∥W.W​+b=1 . The dot product of a vector with itself is the square of its norm so: . W.Xo+m∥W∥2∥W∥+b=1W.X_o + m frac{ |W |^{2}}{ |W |} + b = 1W.Xo​+m∥W∥∥W∥2​+b=1 . W.Xo+m∥W∥+b=1W.X_o + m |W | + b = 1W.Xo​+m∥W∥+b=1 . W.Xo+b=1−m∥W∥W.X_o + b = 1 - m |W |W.Xo​+b=1−m∥W∥ . As $X_o$ is on $H_o$ then $W.X_o + b = -1$ . −1=1−m∥W∥m∥W∥=2-1 = 1 - m |W | m |W | = 2−1=1−m∥W∥m∥W∥=2 . m=2∥W∥m = frac{2}{ |W |}m=∥W∥2​ . So, we got the equation to compute the margin: m=2∥∥W∥∥m = frac{2}{ | |W | |}m=∥∥W∥∥2​. Our goal is to maximize the margin $m$. This could be achieved by minimizing $W$. . Minimize ∥W∥ in (W,b)subject to yi(W.X+b)≥1 (for i=1...n)Minimize space |W |^ space in space (W, b) subject space to space y_{i}(W.X+b) geq 1 space (for space i = 1...n)Minimize ∥W∥ in (W,b)subject to yi​(W.X+b)≥1 (for i=1...n) . Once we minimize the $||W||$, we maximize the margin $m$ and hence we would find our optimal hyperplane lying in middle fof $H_o$ and $H_1$. To make the problem easier when taking the gradients, we will try to minimize the following form: . min 12∥W∥2=min 12WTW s.t. yi(W.X+b)≥1     (for i=1,..,n)                    (7)min space frac{1}{2} |W |^{2} = min space frac{1}{2}W^{T}W space s.t. space y_{i}(W.X+b) geq 1 space space space space space (for space i=1,..,n) space space space space space space space space space space space space space space space space space space space space(7)min 21​∥W∥2=min 21​WTW s.t. yi​(W.X+b)≥1     (for i=1,..,n)                    (7) . Linear Inseparability . All the examples we have seen until so far were linear separable i.e., the data points could be classified with a straight line (or hyperplane). In real world, dataset is not easy to classify with just a hyperplane and are often linear inseparable. Look at one of the examples of linear inseparable data below: . . From the Fig. 13, it is evident that there is no linear decision boundary exists that can perfectly segregate the data i.e. the data is linearly inseparable. We have similar situations in higher-dimensional representations as well. These could be solved easily with either soft-margin formulation or kernel trick. . Soft Margin Formulation . In real life, data is messy and cannot be segregated perfectly with a hyperplane. Due to this, the constraint of maximizing the margin of the line that separates the classes must be relaxed. For example, consider the following diagram: . . In above image, hyperplane 1 perfectly segregates all the training datapoints and respects the constraints of the support vectors i.e. no data point between the support vectors hyperplanes. Due to this constraint, we have a small margin. But, is it really a good idea of having a decision boundary that perfectly separates datapoints with such less margin? The answer is NO. If we consider hyperplane 2 in above image, it gives a bigger margin but have a misclassification of yellow traingle. So which hyperplane is better? Hyperplane 2 is a better decision boundary as it has a wider margin and thus is able to generalize well on the unseen data. Hyperplane 2 though misclassifying one of the datapoints but this error is acceptable in the broad sense of generalizing the data well. . The concept of margins where support vectors doesn&#39;t allow any data points to lie between them is called hard-margin. Hard-margins in SVM are prone to be influenced even by the single outlier (like in image above) which makes the classifier overly sensitive to the noise in data and overfits the data. . Soft-margins in SVM allows the misclassification to a certain extent(hyperplane 2 in above image) while training and deciding decision boundary hyperplane and generalizes well on the data. . How Soft Margin works mathematically? . As explained earlier, for SVM to work, the objective function is equation no. (7). In the new setting of soft margin, objective function from equation no. (7) can be modified as below: . L=12∥W∥2+C(#ofmisclassifications)L = frac{1}{2} |W |^{2} + C( # of misclassifications)L=21​∥W∥2+C(#ofmisclassifications) . This differs from the original objective function in the second term. Here, $C$ is a hyperparameter that decides the trade-off between maximizing the margin and minimizing the misclassifications. When, $C$ is small, classification errors are given less importance and focus is more on maximizing the margin which will lead to SVM overfitting on the dataset. Whereas when $C$ is large, the focus is more on avoiding misclassifications at the expense of keeping the margin small which means that there will be misclassifications allowed while training such that the margin got is not small but big enough to generalize the data very well. . . The above paragraph is little confusing especially last two lines. Do read it again. And again. And again! However, not all misclassification mistakes are equal. Since, decision hyperplane boundary is made based on computation of distances among data points, we also need to consider the distances while considering the misclassification mistakes. Data points that are far away on the wrong side of the decision hyperplane boundary should incur more penalty as compared to the ones that are closer. Let’s see how this is incorporated with the help of following image: . . The idea is: for every data point $X_i$, there is a slack variable introduced $ xi_{i}$. The $ xi_{i}$ represents distance of $X_i$ from the corresponding class’ margin if $X_i$ is on the wrong side of the margin, otherwise zero. Thus the points that are far away from the margin on the wrong side would get more penalty. . With this idea, each data point $X_i$ needs to satisfy the following constraint: . yi(W.X+b)≥1−ξy_i(W.X + b) geq 1 - xiyi​(W.X+b)≥1−ξ . The objective function to minimize with soft-margin setting becomes: . L=min12∥W∥2+C∑i=1nξi     s.t.     yi(W.X+b)≥1−ξi     (for i=1,..,n)L = min frac{1}{2} |W |^{2} + C sum_{i=1}^{n} xi_{i} space space space space space s.t. space space space space space y_i(W.X+b) geq 1 - xi_{i} space space space space space (for space i=1,..,n)L=min21​∥W∥2+Ci=1∑n​ξi​     s.t.     yi​(W.X+b)≥1−ξi​     (for i=1,..,n) . Refereces . SVM Tutorial | Using a Hard Margin vs. Soft Margin in SVM | Internet |",
            "url": "https://theguywithblacktie.github.io/kernel/machine%20learning/2021/06/06/SVM.html",
            "relUrl": "/machine%20learning/2021/06/06/SVM.html",
            "date": " • Jun 6, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Understanding Cross-Entropy Loss and Focal Loss",
            "content": "In this blogpost we will understand cross-entropy loss and its various different names. Later in the post, we will learn about Focal Loss, a successor of Cross-Entropy(CE) loss that performs better than CE in highly imbalanced dataset setting. We will also implement Focal Loss in PyTorch. . Cross-Entropy loss has its different names due to its different variations used in different settings but its core concept (or understanding) remains same across all the different settings. Cross-Entropy Loss is used in a supervised setting and before diving deep into CE, first let’s revise widely known and important concepts: . Classifications . Multi-Class Classification One-of-many classification. Each data point can belong to ONE of CCC classes. The target (ground truth) vector ttt will be a one-hot vector with a positive class and C−1C-1C−1 negative classes. All the CCC classes are mutually exclusives and no two classes can be positive class. The deep learning model will have CCC output neurons depicting probability of each of the CCC class to be positive class and it is gathered in a vector sss (scores). This task is treated as a single classification problem of samples in one of CCC classes. . Multi-Label Classification Each data point can belong to more than one class from CCC classes. The deep learning model will have CCC output neurons. Unlike in multi-class classification, here classes are not mutually exclusive. The target vector ttt can have more than a positive class, so it will be a vector of 000s and 111s with CCC dimensionality where 000 is negative and 111 is positive class. One intutive way to understand multi-label classification is to treat multi-label classification as CCC different binary and independent classification problem where each output neuron decides if a sample belongs to a class or not. . Output Activation Functions . These functions are transformations applied to vectors coming out from the deep learning models before the loss computation. The outputs after transformations represents probabilities of belonging to either one or more classes based on multi-class or multi-label setting. . Sigmoid It squashes a vector in the range (0,1)(0,1)(0,1). It is applied independently to each element of vector sss. . f(si)=11+e−sif(s_i) = frac{1}{1 + e^{-s_{i}}}f(si​)=1+e−si​1​ . Softmax It squashes a vector in the range (0,1)(0, 1)(0,1) and all the resulting elements add up to 111. It is applied to the output vector sss. The Softmax activation cannot be applied independently to each element of vector s, since it depends on all elements of sss. For a given class sis_isi​, the Softmax function can be computed as: . f(s)i=e(si)∑jCesjf(s)_i = frac{e^{(s_i)}}{ sum_{j}^C e^{s_j}}f(s)i​=∑jC​esj​e(si​)​ . Losses . Cross Entropy Loss . The cross-entropy loss is defined as: . CE=−∑iCtilog(si)CE = - sum_i^C t_i log(s_i )CE=−i∑C​ti​log(si​) . where tit_iti​ and sis_isi​ are the goundtruth and output score for each class i in C. . Taking a very rudimentary example, consider the target(groundtruth) vector t and output score vector s as below: . Target Vector: [0.6 0.3 0.1] Score Vector: [0.2 0.3 0.5] . Then CE will be computed as follows: . CE = -(0.6)log(0.2) - 0.3log(0.3) - 0.1log(0.5) = 0.606 . . In supervised machine learning settings, elements of target vectors are either 1 or 0. The above example shows how CE is computed and how it is also applicable to find loss between the distributions. Categorical Cross-Entropy Loss . In multi-class setting, target vector t is one-hot encoded vector with only one positive class (i.e.ti=1t_i = 1ti​=1) and rest are negative class (i.e. ti=0t_i = 0ti​=0). Due to this, we can notice that losses for negative classes are always zero. Hence, it does not make much sense to calculate loss for every class. Whenever our target (ground truth) vector is one-hot vector, we can ignore other labels and utilize only on the hot class for computing cross-entropy loss. So, Cross-Entropy loss becomes: . CE=−log(si)CE = -log(s_i)CE=−log(si​) . . The above form of cross-entropy is called as Categorical Cross-Entropy loss. In multi-class classification, this form is often used for simplicity. The Categorical Cross-Entropy loss is computed as follows: . f(s)i=esi∑jCesj⇒CE=−∑iCtilog(f(s)i)⇒CE=−log(f(s)i)f(s)_i = frac{e^{s_i}}{ sum_{j}^{C}e^{s_j}} Rightarrow CE = - sum_i^C t_i log(f(s)_i) Rightarrow CE = -log(f(s)_i)f(s)i​=∑jC​esj​esi​​⇒CE=−i∑C​ti​log(f(s)i​)⇒CE=−log(f(s)i​) . As, SoftMax activation function is used, many deep learning frameworks and papers often called it as SoftMax Loss as well. . Binary Cross-Entropy Loss . Based on another classification setting, another variant of Cross-Entropy loss exists called as Binary Cross-Entropy Loss(BCE) that is employed during binary classification (C=2)(C = 2)(C=2). Binary classification is multi-class classification with only 2 classes. To dumb it down further, if one class is a negative class automatically the other class becomes positive class. In this classification, the output is not a vector s but just a single value. Let’s understand it further. . The target(ground truth) vector for a random sample contains only one element with value of either 1 or 0. Here, 1 and 0 represents two different classes (C=2)(C = 2)(C=2). The output score value ranges from 0 to 1. If this value is closer to 1 then class 1 is being predicted and if it is closer to 0, class 0 is being predicted. . BCE=−∑i=1C=2tilog(f(s)i)=−t1log(f(s1))−(1−t1)log(1−f(s1))BCE = - sum_{i=1}^{C=2}t_ilog(f(s)_i) = -t_1log(f(s_1)) - (1-t_1)log(1-f(s_1))BCE=−i=1∑C=2​ti​log(f(s)i​)=−t1​log(f(s1​))−(1−t1​)log(1−f(s1​)) . s1s_1s1​ and t1t_1t1​ are the score and groundtruth label for the class CiC_iCi​ in CCC. s2=1−s1s_2 = 1 -s_1s2​=1−s1​ and t2=1−t1t_2 = 1 - t_1t2​=1−t1​ are the score and groundtruth label for the class C2C_2C2​. If t1=0t_1 = 0t1​=0 then −t1log(f(s1))-t_1log(f(s_1))−t1​log(f(s1​)) would become 000 and (1−t1)log(1−f(s1))(1-t_1)log(1-f(s_1))(1−t1​)log(1−f(s1​)) would become active. Similarly, if t1=1t_1 = 1t1​=1 then −t1log(f(s1))-t_1log(f(s_1))−t1​log(f(s1​)) would become active and (1−t1)log(1−f(s1))(1-t_1)log(1-f(s_1))(1−t1​)log(1−f(s1​)) would become 000. The loss can be expressed as: . CE={−log(f(s1))ift1=1−log(1−f(s1))ift1=0CE = begin{cases} -log(f(s_1)) &amp; if &amp; t_1 = 1 -log(1-f(s_1)) &amp; if &amp; t_1 = 0 end{cases}CE={−log(f(s1​))−log(1−f(s1​))​ifif​t1​=1t1​=0​ . To get the output score value between [0,1], sigmoid activation function is used. . . Due to the using of Sigmoid Activation function it is also called as Sigmoid-Cross Entropy Loss. . f(s1)=11+e−s1⇒CE=−t1log(f(s1))−(1−t1)log(1−f(s1))f(s_1) = frac{1}{1+e^{-s_1}} Rightarrow CE = -t_1log(f(s_1)) - (1-t_1)log(1-f(s_1))f(s1​)=1+e−s1​1​⇒CE=−t1​log(f(s1​))−(1−t1​)log(1−f(s1​)) . Cross-Entropy in Multi-Label Classification . As described earlier, in multi-label classification each sample can belong to more than one class. With CCC different classes, multi-label classification is treated as CCC different independent binary classification. Multi-label classification is a binary classification problem w.r.t. every class. The output is vector sss consisting of CCC number of elements. Binary Cross-Entropy Loss is employed in Multi-Label classification and it is computed for each class in each sample. . Loss=∑i=1i=CBCE(ti,f(s)i)=∑i=1i=Ctilog(f(s)i)Loss = sum_{i=1}^{i=C}BCE(t_i, f(s)_i) = sum_{i=1}^{i=C}t_ilog(f(s)_i)Loss=i=1∑i=C​BCE(ti​,f(s)i​)=i=1∑i=C​ti​log(f(s)i​) . Focal Loss . Focal Loss was introduced in Focal Loss for Dense Object Detection paper by He et al (at FAIR). Object detection is one of the most widely studied topics in Computer Vision with a major challenge of detecting small size objects inside images. Object detection algorithms evaluate about 10410^4104 to 10510^5105 candidate locations per image but only a few locations contains objects and rest are just background objects. This leads to class imbalance problem. . Using Binary Cross-Entropy Loss for training with highly class imbalance setting doesn’t perform well. BCE needs the model to be confident about what it is predicting that makes the model learn negative class more easily they are heavily available in the dataset. In short, model learns nothing useful. This can be fixed by Focal Loss, as it makes easier for the model to predict things without being 80−10080-100%80−100 sure that this object is something. Focal Loss allows the model to take risk while making predictions which is highly important when dealing with highly imbalanced datasets. . . Though Focal Loss was introduced with object detection example in paper, Focal Loss is meant to be used when dealing with highly imbalanced datasets. How Focal Loss Works? . Focal Loss is am improved version of Cross-Entropy Loss that tries to handle the class imbalance problem by down-weighting easy negative class and focussing training on hard positive classes. In paper, Focal Loss is mathematically defined as: . FocalLoss=−αt(1−pt)γlog(pt)Focal Loss = - alpha_t(1 - p_t)^{ gamma}log(p_t)FocalLoss=−αt​(1−pt​)γlog(pt​) . . The above definition is Focal Loss for only one class. It has omitted the $$ sum$$ that would sum over all the classes $$C$$. To calculate total Focal Loss per sample, sum over all the classes. What is Alpha and Gamma ? . The only difference between original Cross-Entropy Loss and Focal Loss are these hyperparameters: alpha(α alphaα) and gamma(γ gammaγ). Important point to note is when γ=0 gamma = 0γ=0, Focal Loss becomes Cross-Entropy Loss. . Let’s understand the graph below which shows what influences hyperparameters α alphaα and γ gammaγ has on Focal Loss and in turn understand them. In the graph, “blue” line represents Cross-Entropy Loss. The X-axis or “probability of ground truth class” (let’s call it pt) is the probability that the model predicts for the ground truth object. As an example, let’s say the model predicts that something is a bike with probability 0.60.60.6 and it actually is a bike. In this case, pt is 0.60.60.6. In the case when object is not a bike, the pt is 0.4(1−0.6)0.4 (1-0.6)0.4(1−0.6). The Y-axis denotes the loss values at a given pt. . As can be seen from the image, when the model predicts the ground truth with a probability of 0.60.60.6, the Cross-Entropy Loss is somewhere around 0.50.50.5. Therefore, to reduce the loss, the model would have to predict the ground truth class with a much higher probability. In other words, Cross-Entropy Loss asks the model to be very confident about the ground truth prediction. . This in turn can actually impact the performance negatively: . The deep learning model can actually become overconfident and therefore, the model wouldn’t generalize well. . Focal Loss helps here. As can be seen from the graph, Focal Loss with γ&gt;1 gamma &gt; 1γ&gt;1 reduces the loss for “well-classified examples” or examples when the model predicts the right thing with probability &gt;0.5&gt; 0.5&gt;0.5 whereas, it increases loss for “hard-to-classify examples” when the model predicts with probability &lt;0.5&lt;0.5&lt;0.5. Therefore, it turns the model’s attention towards the rare class in case of class imbalance. . γ gammaγ controls the shape of curve. The higher the value of γ gammaγ, the lower the loss for well-classified examples, so we could turn the attention of the model towards ‘hard-to-classify’ examples. Having higher γ gammaγ extends the range in which an example receives low loss. . Another way, apart from Focal Loss, to deal with class imbalance is to introduce weights. Give high weights to the rare class and small weights to the common classes. These weights are referred as α alphaα. . But Focal Loss paper notably states that adding different weights to different classes to balance the class imbalance is not enough. We also need to reduce the loss of easily-classified examples to avoid them dominating the training. To deal with this, multiplicative factor (1−pt)γ(1-p_t)^{ gamma}(1−pt​)γ is added to Cross-Entropy Loss which gives the Focal Loss. . Focal Loss: Code Implementation . Here is the implementation of Focal Loss in PyTorch: . class WeightedFocalLoss(nn.Module): def __init__(self, batch_size, alpha=0.25, gamma=2): super(WeightedFocalLoss, self).__init__() self.alpha = alpha.repeat(batch_size, 1) self.gamma = gamma def forward(self, inputs, targets): BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=&#39;none&#39;) targets = targets.type(torch.long) at = self.alpha pt = torch.exp(-BCE_loss) F_loss = at*(1-pt)**self.gamma * BCE_loss return F_loss.mean() . References . Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names | What is Focal Loss and when should you use it? | A Beginner’s Guide to Focal Loss in Object Detection! |",
            "url": "https://theguywithblacktie.github.io/kernel/machine%20learning/pytorch/2021/05/20/cross-entropy-loss.html",
            "relUrl": "/machine%20learning/pytorch/2021/05/20/cross-entropy-loss.html",
            "date": " • May 20, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Adding Variable Number of Layers in Neural Network",
            "content": "Consider following code block that defines a fixed 2-layer neural network. Imagine a scenario, where the network has huge number of layers, and typing out each layer manually is just not feasible. An even more notable scenario is when the number of layers of network are not fixed and it depends on some other conigurations. This article deals with these scenarios and lays out solution. . class Net(nn.Module): def __init__(self, input_dim, output_dim): super(Net, self).__init__() linear_layer_one = nn.Linear(input_dim, output_dim) linear_layer_two = nn.Linear(input_dim, output_dim) def forward(self, input): output = linear_layer_two( linear_layer_one(input) ) return output . Recently, I was implementing a library related to Graph Networks in PyTorch framework. There I encountered the second scenario where the number of layers in the neural network were not fixed and it network required number of layers to be added as an input. The user of the library would specify how many layers is required as the input and that many layers would be added in the neural network. . As a daily user of Python, my first solution was to use a list data structure with a for loop to add n-number of layers, like below code block. . class Net(nn.Module): def __init__(self, input_dim, output_dim, nos_linear_layer): super(Net, self).__init__() self.nn_layers = [] for i in range(0,nos_linear_layer): linear_layer = nn.Linear(input_dim, output_dim) self.nn_layers.append(linear_layer) def forward(self, input): outputs = None for i,layer in enumerate(self.nn_layers): outputs = layer(input) outputs = torch.nn.functional.Softmax(outputs, 1) return outputs . Above code would look correct and would be expected to run without any issue. But, the main issue is that the linear layers stored in Python list would not be trained. On calling model.parameters(), PyTorch would simply ignore the parameters of linear layers stored in the Python list. . The correct way is to use PyTorch’s list nn.ModuleList. . class Net(nn.Module): def __init__(self, input_dim, output_dim, nos_linear_layer): super(Net, self).__init__() self.nn_layers = nn.ModuleList() for i in range(0,nos_linear_layer): linear_layer = nn.Linear(input_dim, output_dim) self.nn_layers.append(linear_layer) def forward(self, input): outputs = None for i,layer in enumerate(self.nn_layers): outputs = layer(input) outputs = torch.nn.functional.Softmax(outputs, 1) return outputs .",
            "url": "https://theguywithblacktie.github.io/kernel/pytorch/2021/05/14/vary-layers-pytorch.html",
            "relUrl": "/pytorch/2021/05/14/vary-layers-pytorch.html",
            "date": " • May 14, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://theguywithblacktie.github.io/kernel/2020/01/14/test-markdown-post.html",
            "relUrl": "/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is Ashish! While working in software engineering for 3 years, I pondered upon Machine Learning course by Andrew Ng and since then my daily learnings have never been the same. Machine Learning or AI (as many calls it for sophistication) is a vast field that is growing exponentially even today. Hence, I am documenting my learning notes in this blog to help out people and me in future looking for answers. Reach out to me on my social handles or via email incase you have any questions or find errors. Thanks :) . If you like any of my blog or found it helpful in your ML journey, do let me know by messaging me on social media. Otherwise you could also share your appreciation by sharing the work on social media and taging me along with it. These would really encourage me to write even more quality blogs. .",
          "url": "https://theguywithblacktie.github.io/kernel/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://theguywithblacktie.github.io/kernel/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}