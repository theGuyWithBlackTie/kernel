<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://theguywithblacktie.github.io/kernel/feed.xml" rel="self" type="application/atom+xml" /><link href="https://theguywithblacktie.github.io/kernel/" rel="alternate" type="text/html" /><updated>2021-11-08T13:29:56-06:00</updated><id>https://theguywithblacktie.github.io/kernel/feed.xml</id><title type="html">Multiplying Matrices for a living</title><subtitle>NLP in day, CV by night.\ Documenting my learnings of Machine Learning.\ Motto: Be Limitless
</subtitle><entry><title type="html">Understanding Support Vector Machine</title><link href="https://theguywithblacktie.github.io/kernel/machine%20learning/2021/06/06/SVM.html" rel="alternate" type="text/html" title="Understanding Support Vector Machine" /><published>2021-06-06T00:00:00-05:00</published><updated>2021-06-06T00:00:00-05:00</updated><id>https://theguywithblacktie.github.io/kernel/machine%20learning/2021/06/06/SVM</id><author><name></name></author><category term="Machine Learning" /><summary type="html">In Machine Learning, out of the many available supervised classification algorithms, Support Vector Machine (SVM) is one of the easiest algorithm and yet sometimes it becomes difficult to grasp due to its little nuances. In this blog, I try to lay down my notes on unboxing the SVM blackbox and make it easy to understand in detail. This blog focusses on entirety of SVM, from its introduction to classification and kernel tricks.</summary></entry><entry><title type="html">Understanding Cross-Entropy Loss and Focal Loss</title><link href="https://theguywithblacktie.github.io/kernel/machine%20learning/pytorch/2021/05/20/cross-entropy-loss.html" rel="alternate" type="text/html" title="Understanding Cross-Entropy Loss and Focal Loss" /><published>2021-05-20T00:00:00-05:00</published><updated>2021-05-20T00:00:00-05:00</updated><id>https://theguywithblacktie.github.io/kernel/machine%20learning/pytorch/2021/05/20/cross-entropy-loss</id><author><name></name></author><category term="Machine Learning" /><category term="PyTorch" /><summary type="html">In this blogpost we will understand cross-entropy loss and its various different names. Later in the post, we will learn about Focal Loss, a successor of Cross-Entropy(CE) loss that performs better than CE in highly imbalanced dataset setting. We will also implement Focal Loss in PyTorch.</summary></entry><entry><title type="html">Adding Variable Number of Layers in Neural Network</title><link href="https://theguywithblacktie.github.io/kernel/pytorch/2021/05/14/vary-layers-pytorch.html" rel="alternate" type="text/html" title="Adding Variable Number of Layers in Neural Network" /><published>2021-05-14T00:00:00-05:00</published><updated>2021-05-14T00:00:00-05:00</updated><id>https://theguywithblacktie.github.io/kernel/pytorch/2021/05/14/vary-layers-pytorch</id><author><name></name></author><category term="PyTorch" /><summary type="html">Consider following code block that defines a fixed 2-layer neural network. Imagine a scenario, where the network has huge number of layers, and typing out each layer manually is just not feasible. An even more notable scenario is when the number of layers of network are not fixed and it depends on some other conigurations. This article deals with these scenarios and lays out solution.</summary></entry><entry><title type="html">An Example Markdown Post</title><link href="https://theguywithblacktie.github.io/kernel/2020/01/14/test-markdown-post.html" rel="alternate" type="text/html" title="An Example Markdown Post" /><published>2020-01-14T00:00:00-06:00</published><updated>2020-01-14T00:00:00-06:00</updated><id>https://theguywithblacktie.github.io/kernel/2020/01/14/test-markdown-post</id><author><name></name></author><summary type="html">Example Markdown Post</summary></entry></feed>