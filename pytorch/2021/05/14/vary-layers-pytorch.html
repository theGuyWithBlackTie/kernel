<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Adding Variable Number of Layers in Neural Network | Multiplying Matrices for a living</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Adding Variable Number of Layers in Neural Network" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Code example of showing right way parameterize number of layers to be added in Neural Network" />
<meta property="og:description" content="Code example of showing right way parameterize number of layers to be added in Neural Network" />
<link rel="canonical" href="https://theguywithblacktie.github.io/kernel/pytorch/2021/05/14/vary-layers-pytorch.html" />
<meta property="og:url" content="https://theguywithblacktie.github.io/kernel/pytorch/2021/05/14/vary-layers-pytorch.html" />
<meta property="og:site_name" content="Multiplying Matrices for a living" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-05-14T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2021-05-14T00:00:00-05:00","url":"https://theguywithblacktie.github.io/kernel/pytorch/2021/05/14/vary-layers-pytorch.html","@type":"BlogPosting","headline":"Adding Variable Number of Layers in Neural Network","dateModified":"2021-05-14T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://theguywithblacktie.github.io/kernel/pytorch/2021/05/14/vary-layers-pytorch.html"},"description":"Code example of showing right way parameterize number of layers to be added in Neural Network","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/kernel/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://theguywithblacktie.github.io/kernel/feed.xml" title="Multiplying Matrices for a living" /><link rel="shortcut icon" type="image/x-icon" href="/kernel/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/kernel/">Multiplying Matrices for a living</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/kernel/about/">About Me</a><a class="page-link" href="/kernel/search/">Search</a><a class="page-link" href="/kernel/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Adding Variable Number of Layers in Neural Network</h1><p class="page-description">Code example of showing right way parameterize number of layers to be added in Neural Network</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-05-14T00:00:00-05:00" itemprop="datePublished">
        May 14, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      1 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/kernel/categories/#pytorch">pytorch</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><p>Consider following code block that defines a fixed 2-layer neural network. Imagine a scenario, where the network has huge number of layers, and typing out each layer manually is just not feasible. An even more notable scenario is when the number of layers of network are not fixed and it depends on some other conigurations. This article deals with these scenarios and lays out solution.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="n">linear_layer_one</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
    <span class="n">linear_layer_two</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
    
   <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">linear_layer_two</span><span class="p">(</span> <span class="n">linear_layer_one</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<p>Recently, I was implementing a library related to Graph Networks in PyTorch framework. There I encountered the second scenario where the number of layers in the neural network were not fixed and it network required number of layers to be added as an input. The user of the library would specify how many layers is required as the input and that many layers would be added in the neural network.</p>

<p>As a daily user of Python, my first solution was to use a <code class="language-plaintext highlighter-rouge">list</code> data structure with a <code class="language-plaintext highlighter-rouge">for</code> loop to add n-number of layers, like below code block.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">nos_linear_layer</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">nn_layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nos_linear_layer</span><span class="p">):</span>
      <span class="n">linear_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
      <span class="bp">self</span><span class="p">.</span><span class="n">nn_layers</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">linear_layer</span><span class="p">)</span>
    
   <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">nn_layers</span><span class="p">):</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div>

<p>Above code would look correct and would be expected to run without any issue. But, the main issue is that the linear layers stored in Python <code class="language-plaintext highlighter-rouge">list</code> would not be trained. On calling <code class="language-plaintext highlighter-rouge">model.parameters()</code>, PyTorch would simply ignore the parameters of linear layers stored in the Python <code class="language-plaintext highlighter-rouge">list</code>.</p>

<p>The correct way is to use PyTorch’s list <code class="language-plaintext highlighter-rouge">nn.ModuleList</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">nos_linear_layer</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">nn_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nos_linear_layer</span><span class="p">):</span>
      <span class="n">linear_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
      <span class="bp">self</span><span class="p">.</span><span class="n">nn_layers</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">linear_layer</span><span class="p">)</span>
    
   <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">nn_layers</span><span class="p">):</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div>

  </div><a class="u-url" href="/kernel/pytorch/2021/05/14/vary-layers-pytorch.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/kernel/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/kernel/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/kernel/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>NLP in day, CV by night.\ Documenting my learnings of Machine Learning.\ Motto: Be Limitless
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/theguywithblacktie" title="theguywithblacktie"><svg class="svg-icon grey"><use xlink:href="/kernel/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/aasinghal" title="aasinghal"><svg class="svg-icon grey"><use xlink:href="/kernel/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/guywithblacktie" title="guywithblacktie"><svg class="svg-icon grey"><use xlink:href="/kernel/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
